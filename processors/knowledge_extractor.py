#!/usr/bin/env python3
"""
çŸ¥è­˜æå–å™¨
å¾é€å­—ç¨¿ä¸­æå–å•†æ¥­çŸ¥è­˜
"""

import os
from pathlib import Path
from typing import Optional, Dict, List
from datetime import datetime

# å°å…¥æœ¬åœ°æ¨¡çµ„
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from processors.llm_client import get_llm_client


class KnowledgeExtractor:
    """å•†æ¥­çŸ¥è­˜æå–å™¨"""
    
    def __init__(self, prompts_dir: str = None):
        if prompts_dir is None:
            prompts_dir = Path(__file__).parent.parent / "config" / "prompts"
        self.prompts_dir = Path(prompts_dir)
        self.llm = get_llm_client()
        
        # è¼‰å…¥ Prompts
        self.knowledge_prompt = self._load_prompt("knowledge_extraction.txt")
        self.speaker_prompt = self._load_prompt("speaker_identification.txt")
    
    def _load_prompt(self, filename: str) -> str:
        """è¼‰å…¥ Prompt æ¨¡æ¿"""
        prompt_file = self.prompts_dir / filename
        if prompt_file.exists():
            return prompt_file.read_text(encoding='utf-8')
        return ""
    
    def identify_speakers(self, transcript: str, video_info: Dict = None) -> str:
        """
        è­˜åˆ¥è¬›è€…ï¼ˆä½¿ç”¨å½±ç‰‡å…ƒæ•¸æ“šè¼”åŠ©è­˜åˆ¥ï¼‰
        
        Args:
            transcript: åŸå§‹é€å­—ç¨¿
            video_info: å½±ç‰‡è³‡è¨Š {'title', 'channel', 'description'}
            
        Returns:
            æ¨™è¨˜è¬›è€…å¾Œçš„é€å­—ç¨¿
        """
        # å¾å½±ç‰‡å…ƒæ•¸æ“šæå–è¬›è€…è³‡è¨Š
        speaker_hints = ""
        if video_info:
            channel = video_info.get('channel', '')
            title = video_info.get('title', '')
            description = video_info.get('description', '')[:500] if video_info.get('description') else ''
            
            speaker_hints = f"""
## å·²çŸ¥è¬›è€…è³‡è¨Šï¼ˆè«‹å„ªå…ˆä½¿ç”¨ï¼‰

- **é »é“ä¸»æŒäºº/ä¸»è¬›è€…**: {channel}
- **å½±ç‰‡æ¨™é¡Œ**: {title}
- **æè¿°æ‘˜è¦**: {description[:200] if description else 'ç„¡'}

### è­˜åˆ¥è¦å‰‡
1. è‹¥ç‚ºå–®äººå½±ç‰‡ï¼ˆVlogã€æ•™å­¸ï¼‰ï¼Œä¸»è¬›è€…ç‚ºé »é“æ“æœ‰è€…ã€Œ{channel}ã€
2. è‹¥ç‚ºè¨ªè«‡ï¼Œä¸»æŒäººé€šå¸¸æ˜¯é »é“æ“æœ‰è€…ã€Œ{channel}ã€
3. è¨ªè«‡å˜‰è³“å§“åå¯èƒ½å‡ºç¾åœ¨æ¨™é¡Œæˆ–æè¿°ä¸­
4. **ç¦æ­¢ä½¿ç”¨è™›æ§‹æˆ–ä½”ä½ç¬¦å§“å**ï¼ˆå¦‚ Cortexã€å¼µä¸‰ç­‰ï¼‰
5. ç„¡æ³•è­˜åˆ¥æ™‚ç”¨ã€Œä¸»è¬›è€…ã€æˆ–ã€Œå˜‰è³“ã€ä»£æ›¿
"""

        prompt = f"""
{self.speaker_prompt}

{speaker_hints}

## å¾…åˆ†æé€å­—ç¨¿

{transcript[:8000]}
"""
        
        result = self.llm.generate(
            prompt=prompt,
            system_prompt=f"ä½ æ˜¯å°ˆæ¥­çš„èªéŸ³åˆ†æå¸«ã€‚æ­¤å½±ç‰‡ä¾†è‡ªé »é“ã€Œ{video_info.get('channel', 'æœªçŸ¥')}ã€ï¼Œè«‹è­˜åˆ¥å°è©±ä¸­çš„ä¸åŒè¬›è€…ã€‚",
            max_tokens=8000,
            temperature=0.3
        )
        
        return result if result else transcript
    
    def _load_ontology_entities(self) -> List[str]:
        """è¼‰å…¥æœ¬é«”è«–å¯¦é«”æ¸…å–® (80/20 å„ªåŒ–)"""
        ontology_path = Path.home() / "R2R/config/ontology/solo_entrepreneur_synonyms.json"
        try:
            if ontology_path.exists():
                import json
                with open(ontology_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                # æå–æ‰€æœ‰ entity çš„ name
                return [item.get("name", "") for item in data if item.get("name")]
        except Exception:
            pass
        return []
    
    def _is_interview_content(self, transcript: str, video_info: Dict = None) -> bool:
        """
        é æª¢ï¼šåˆ¤æ–·æ˜¯å¦ç‚ºè¨ªè«‡å…§å®¹ (ä¸èª¿ç”¨ LLMï¼Œç¯€ç´„ API)
        
        æª¢é©—æ©Ÿåˆ¶ï¼š
        1. æ¨™é¡Œé—œéµå­—æª¢æŸ¥
        2. é€å­—ç¨¿å…§å®¹é—œéµå­—æª¢æŸ¥
        
        Returns:
            True = å¯èƒ½æ˜¯è¨ªè«‡ï¼Œéœ€è¦è­˜åˆ¥ guest
            False = éè¨ªè«‡ï¼Œè·³é guest è­˜åˆ¥
        """
        # è¨ªè«‡ç›¸é—œé—œéµå­—
        interview_keywords = [
            'è¨ªè«‡', 'å°ˆè¨ª', 'å°è«‡', 'å°è©±', 'è¨ªå•', 'è«‹åˆ°', 'é‚€è«‹',
            'å˜‰è³“', 'è€å¸«', 'ä¾†è³“', 'ç‰¹åˆ¥å˜‰è³“',
            'interview', 'podcast', 'guest', 'feat', 'ft.', 'with',
            'q&a', 'qa', 'å•ç­”', 'é€£ç·š'
        ]
        
        # 1. æª¢æŸ¥æ¨™é¡Œ
        title = ""
        if video_info:
            title = video_info.get('title', '').lower()
        
        for keyword in interview_keywords:
            if keyword.lower() in title:
                return True
        
        # 2. æª¢æŸ¥é€å­—ç¨¿å‰ 500 å­—
        transcript_head = transcript[:500].lower() if transcript else ""
        
        # è¨ªè«‡é–‹å ´ç‰¹å¾µ
        interview_patterns = [
            'ä»Šå¤©æˆ‘å€‘è«‹åˆ°', 'ä»Šå¤©çš„å˜‰è³“', 'ä»Šå¤©é‚€è«‹', 'æ­¡è¿ä¾†åˆ°',
            'è«‹å•', 'å¯ä»¥ä»‹ç´¹ä¸€ä¸‹', 'å…ˆåšå€‹è‡ªæˆ‘ä»‹ç´¹',
            'è¬è¬é‚€è«‹', 'å¾ˆé«˜èˆˆä¾†åˆ°', 'æ„Ÿè¬ä¸»æŒäºº',
            "today's guest", "welcome to the show", "thanks for having me"
        ]
        
        for pattern in interview_patterns:
            if pattern.lower() in transcript_head:
                return True
        
        return False
    
    def extract_knowledge(self, transcript: str, video_info: Dict = None) -> Dict:
        """
        æå–å•†æ¥­çŸ¥è­˜ï¼ˆåˆä½µèª¿ç”¨ï¼šçŸ¥è­˜ + æ‘˜è¦ + é—œéµå­— + å¯¦é«” + æ¨™ç±¤ + å˜‰è³“ï¼‰
        
        80/20 å„ªåŒ–ï¼šåœ¨æºé ­ä¸€æ¬¡å®Œæˆæ‰€æœ‰æå–ï¼Œé¿å… R2R Phase1 é‡è¤‡ API èª¿ç”¨
        è¨ªè«‡è­˜åˆ¥ï¼šé æª¢æ©Ÿåˆ¶ç¯€ç´„ APIï¼Œåªåœ¨è¨ªè«‡å…§å®¹æ™‚æå– guest
        
        Args:
            transcript: é€å­—ç¨¿ (å·²æ¨™è¨˜è¬›è€…)
            video_info: å½±ç‰‡è³‡è¨Š {'title': ..., 'url': ..., 'duration': ...}
            
        Returns:
            æå–çš„çŸ¥è­˜ {'summary': ..., 'knowledge': ..., 'keywords': ..., 'entities': ..., 'tags': ..., 'guest': ...}
        """
        # æ™ºèƒ½æˆªæ–·ï¼šç§»é™¤é‡è¤‡è¡Œ
        lines = transcript.split('\n')
        unique_lines = list(dict.fromkeys(lines))
        clean_transcript = '\n'.join([l for l in unique_lines if len(l.strip()) > 5])[:10000]
        
        # è¼‰å…¥æœ¬é«”è«–å¯¦é«” (80/20 å„ªåŒ–)
        ontology_entities = self._load_ontology_entities()
        ontology_hint = ""
        if ontology_entities:
            ontology_hint = f"""
### å¯¦é«” (Entities)
è«‹å¾ä»¥ä¸‹é å®šç¾©å¯¦é«”ä¸­é¸æ“‡ 3-8 å€‹æœ€åŒ¹é…çš„ï¼š
{', '.join(ontology_entities[:80])}...

è«‹åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- ENTITIES: ["å¯¦é«”1", "å¯¦é«”2", ...] -->`

### æ¨™ç±¤ (Tags)
è«‹ç”Ÿæˆ 3-5 å€‹é©åˆç”¨æ–¼ç­†è¨˜åˆ†é¡çš„æ¨™ç±¤ï¼ˆä¸å« #ï¼‰ï¼š
è«‹åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- TAGS: ["æ¨™ç±¤1", "æ¨™ç±¤2", ...] -->`
"""
        
        # è¨ªè«‡å˜‰è³“è­˜åˆ¥ (é æª¢æ©Ÿåˆ¶ç¯€ç´„ API)
        is_interview = self._is_interview_content(clean_transcript, video_info)
        guest_hint = ""
        if is_interview:
            guest_hint = """
### è¨ªè«‡å˜‰è³“ (Guest)
å¦‚æœé€™æ˜¯è¨ªè«‡/å°è«‡å…§å®¹ï¼Œè«‹è­˜åˆ¥å—è¨ªè€…/å˜‰è³“å§“åã€‚
æ³¨æ„ï¼šä¸»æŒäºº/é »é“ä¸»ä¸ç®—å˜‰è³“ï¼Œåªè­˜åˆ¥è¢«é‚€è«‹çš„ä¾†è³“ã€‚
å¦‚ç„¡å˜‰è³“æˆ–ç„¡æ³•è­˜åˆ¥ï¼Œè«‹è¼¸å‡ºç©ºå­—ä¸²ã€‚
è«‹åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- GUEST: "å˜‰è³“å§“å" -->`
"""
        
        # æº–å‚™ä¸Šä¸‹æ–‡
        context = ""
        if video_info:
            context = f"""
## å½±ç‰‡è³‡è¨Š
- æ¨™é¡Œ: {video_info.get('title', 'æœªçŸ¥')}
- ä¾†æº: {video_info.get('channel', 'æœªçŸ¥')}
- æ™‚é•·: {video_info.get('duration', 'æœªçŸ¥')}
"""
        
        # åˆä½µ Promptï¼šçŸ¥è­˜æå– + æ‘˜è¦ + é—œéµå­— + å¯¦é«” + æ¨™ç±¤ + å˜‰è³“
        prompt = f"""
{self.knowledge_prompt}

{context}

## é€å­—ç¨¿å…§å®¹

{clean_transcript}

---

## é¡å¤–è¼¸å‡ºï¼ˆè«‹åœ¨çŸ¥è­˜æå–å¾Œæ·»åŠ ï¼‰

### ä¸€å¥è©±æ‘˜è¦
è«‹åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- SUMMARY: [ä¸è¶…é100å­—çš„æ ¸å¿ƒè§€é»æ‘˜è¦] -->`

### é—œéµå­—
è«‹åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- KEYWORDS: ["é—œéµå­—1", "é—œéµå­—2", ...] -->`

{ontology_hint}
{guest_hint}
"""
        
        result_text = self.llm.generate(
            prompt=prompt,
            system_prompt="ä½ æ˜¯å•†æ¥­çŸ¥è­˜æå–å°ˆå®¶ã€‚è«‹å¾é€å­—ç¨¿ä¸­æå–çŸ¥è­˜ï¼Œä¸¦åœ¨æ–‡æœ«æŒ‰æŒ‡å®šæ ¼å¼æ·»åŠ æ‘˜è¦ã€é—œéµå­—ã€å¯¦é«”å’Œæ¨™ç±¤ã€‚",
            max_tokens=4500,
            temperature=0.5
        )
        
        if not result_text:
            return {"error": "çŸ¥è­˜æå–å¤±æ•—"}
        
        # è§£æåˆä½µçµæœ
        summary = ""
        keywords = []
        entities = []
        tags = []
        knowledge = result_text
        
        # æå–æ‘˜è¦
        import re
        import json
        
        summary_match = re.search(r'<!-- SUMMARY: (.+?) -->', result_text)
        if summary_match:
            summary = summary_match.group(1).strip()
            knowledge = knowledge.replace(summary_match.group(0), '')
        
        # æå–é—œéµå­—
        keywords_match = re.search(r'<!-- KEYWORDS: (\[.+?\]) -->', result_text)
        if keywords_match:
            try:
                keywords = json.loads(keywords_match.group(1))
                knowledge = knowledge.replace(keywords_match.group(0), '')
            except:
                pass
        
        # æå–å¯¦é«” (80/20 å„ªåŒ–)
        entities_match = re.search(r'<!-- ENTITIES: (\[.+?\]) -->', result_text)
        if entities_match:
            try:
                entities = json.loads(entities_match.group(1))
                knowledge = knowledge.replace(entities_match.group(0), '')
            except:
                pass
        
        # æå–æ¨™ç±¤ (80/20 å„ªåŒ–)
        tags_match = re.search(r'<!-- TAGS: (\[.+?\]) -->', result_text)
        if tags_match:
            try:
                tags = json.loads(tags_match.group(1))
                knowledge = knowledge.replace(tags_match.group(0), '')
            except:
                pass
        
        # æå–è¨ªè«‡å˜‰è³“ (æ¢ä»¶å¼ï¼šåªåœ¨è¨ªè«‡å…§å®¹æ™‚æå–)
        guest = None
        guest_match = re.search(r'<!-- GUEST: "(.+?)" -->', result_text)
        if guest_match:
            guest = guest_match.group(1).strip()
            if guest and guest not in ['', 'ç„¡', 'None', 'null', 'ç„¡æ³•è­˜åˆ¥']:
                knowledge = knowledge.replace(guest_match.group(0), '')
            else:
                guest = None
        
        return {
            "knowledge": knowledge.strip(),
            "summary": summary,
            "keywords": keywords,
            "entities": entities,
            "tags": tags,
            "guest": guest,  # æ–°å¢ï¼šè¨ªè«‡å˜‰è³“
            "metadata": {
                "processed_at": datetime.now().isoformat(),
                "llm_provider": self.llm.current_provider,
                "video_info": video_info,
                "optimized": True,
                "ontology_used": len(ontology_entities) > 0,
                "is_interview": is_interview  # æ¨™è¨˜æ˜¯å¦åµæ¸¬ç‚ºè¨ªè«‡
            }
        }
    def _should_skip_speaker_id(self, video_info: Dict) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦è·³éè¬›è€…è­˜åˆ¥ï¼ˆå„ªåŒ– API èª¿ç”¨ï¼‰
        
        è·³éæ¢ä»¶ï¼š
        - æ¨™é¡Œä¸åŒ…å«è¨ªè«‡ç›¸é—œè©å½™
        - éæ˜é¡¯å¤šäººå°è©±å…§å®¹
        """
        if not video_info:
            return False
        
        title = video_info.get('title', '').lower()
        
        # è¨ªè«‡ç›¸é—œé—œéµå­—ï¼ˆéœ€è¦è¬›è€…è­˜åˆ¥ï¼‰
        interview_keywords = [
            'è¨ªè«‡', 'å°ˆè¨ª', 'å°è«‡', 'å°è©±', 'interview', 'podcast', 
            'å˜‰è³“', 'guest', 'feat', 'ft.', 'ft', 'with', 'èˆ‡', 'å’Œ',
            'q&a', 'qa', 'å•ç­”'
        ]
        
        # å¦‚æœæ¨™é¡ŒåŒ…å«è¨ªè«‡é—œéµå­—ï¼Œä¸è·³é
        for keyword in interview_keywords:
            if keyword in title:
                return False
        
        # å–®äººå…§å®¹é—œéµå­—ï¼ˆå¯è·³éè¬›è€…è­˜åˆ¥ï¼‰
        solo_keywords = [
            'vlog', 'æ•™å­¸', 'tutorial', 'guide', 'åˆ†äº«', 'å¿ƒå¾—',
            'review', 'è©•æ¸¬', 'é–‹ç®±', 'unbox', 'æ—¥å¸¸', 'routine'
        ]
        
        for keyword in solo_keywords:
            if keyword in title:
                return True
        
        # é è¨­ï¼šä¸è·³éï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        return False
    
    def process_transcript(self, transcript: str, video_info: Dict = None) -> Dict:
        """
        å®Œæ•´è™•ç†é€å­—ç¨¿ï¼ˆå„ªåŒ–ç‰ˆï¼‰
        
        Args:
            transcript: åŸå§‹é€å­—ç¨¿
            video_info: å½±ç‰‡è³‡è¨Š
            
        Returns:
            è™•ç†çµæœ
        """
        print("ğŸ” é–‹å§‹è™•ç†é€å­—ç¨¿...")
        
        # 1. æ™ºèƒ½åˆ¤æ–·æ˜¯å¦éœ€è¦è¬›è€…è­˜åˆ¥
        if self._should_skip_speaker_id(video_info):
            print("   âš¡ è·³éè¬›è€…è­˜åˆ¥ï¼ˆå–®äººå…§å®¹ï¼‰")
            marked_transcript = transcript
        else:
            print("   ğŸ‘¥ è­˜åˆ¥è¬›è€…...")
            marked_transcript = self.identify_speakers(transcript, video_info)
        
        # 2. æå–çŸ¥è­˜ï¼ˆå·²åˆä½µæ‘˜è¦å’Œé—œéµå­—ï¼‰
        print("   ğŸ“š æå–å•†æ¥­çŸ¥è­˜...")
        result = self.extract_knowledge(marked_transcript, video_info)
        
        # 3. æ·»åŠ æ¨™è¨˜å¾Œçš„é€å­—ç¨¿
        result["marked_transcript"] = marked_transcript
        
        print("âœ… è™•ç†å®Œæˆ!")
        return result


if __name__ == "__main__":
    print("ğŸ§  MediaMiner Knowledge Extractor")
    print("=" * 50)
    
    extractor = KnowledgeExtractor()
    
    # æ¸¬è©¦æ–‡æœ¬
    test_transcript = """
    ä¸»æŒäººï¼šå¤§å®¶å¥½ï¼Œæ­¡è¿ä¾†åˆ°ä»Šå¤©çš„ç¯€ç›®ã€‚ä»Šå¤©æˆ‘å€‘é‚€è«‹åˆ°äº†çŸ¥åå‰µæ¥­è€…å¼µå…ˆç”Ÿã€‚
    å¼µå…ˆç”Ÿï¼šè¬è¬é‚€è«‹ã€‚
    ä¸»æŒäººï¼šæ‚¨èƒ½è·Ÿæˆ‘å€‘åˆ†äº«ä¸€ä¸‹å‰µæ¥­åˆæœŸæœ€é‡è¦çš„æ˜¯ä»€éº¼å—ï¼Ÿ
    å¼µå…ˆç”Ÿï¼šæˆ‘èªç‚ºæœ€é‡è¦çš„æ˜¯æ‰¾åˆ°ç”¢å“å¸‚å ´åŒ¹é…ã€‚å¾ˆå¤šå‰µæ¥­è€…ä¸€é–‹å§‹å°±æƒ³è‘—æ“´å¼µï¼Œ
    ä½†å…¶å¯¦æ‡‰è©²å…ˆé©—è­‰ä½ çš„ç”¢å“æ˜¯å¦çœŸæ­£è§£æ±ºäº†ç”¨æˆ¶çš„ç—›é»ã€‚
    """
    
    result = extractor.process_transcript(
        test_transcript,
        {"title": "å‰µæ¥­è¨ªè«‡", "channel": "æ¸¬è©¦é »é“"}
    )
    
    print("\nğŸ“Š çµæœ:")
    print(f"æ‘˜è¦: {result.get('summary', 'N/A')}")
    print(f"é—œéµå­—: {result.get('keywords', [])}")
