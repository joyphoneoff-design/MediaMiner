#!/usr/bin/env python3
"""
çŸ¥è­˜æå–å™¨
å¾é€å­—ç¨¿ä¸­æå–å•†æ¥­çŸ¥è­˜
"""

import os
from pathlib import Path
from typing import Optional, Dict, List
from datetime import datetime

# å°å…¥æœ¬åœ°æ¨¡çµ„
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from processors.llm_client import get_llm_client


class KnowledgeExtractor:
    """å•†æ¥­çŸ¥è­˜æå–å™¨"""
    
    def __init__(self, prompts_dir: str = None):
        if prompts_dir is None:
            prompts_dir = Path(__file__).parent.parent / "config" / "prompts"
        self.prompts_dir = Path(prompts_dir)
        self.llm = get_llm_client()
        
        # è¼‰å…¥ Prompts
        self.knowledge_prompt = self._load_prompt("knowledge_extraction.txt")
        self.speaker_prompt = self._load_prompt("speaker_identification.txt")
    
    def _load_prompt(self, filename: str) -> str:
        """è¼‰å…¥ Prompt æ¨¡æ¿"""
        prompt_file = self.prompts_dir / filename
        if prompt_file.exists():
            return prompt_file.read_text(encoding='utf-8')
        return ""
    
    def _smart_sample(self, content: str, target_length: int = None) -> str:
        """
        æ™ºæ…§æ¡æ¨£ï¼šé–‹é ­ + ä¸­é–“ + çµå°¾
        
        80/20 å„ªåŒ–ï¼šä»¥æœ€å°æˆæœ¬ï¼ˆä¸å¢åŠ  tokenï¼‰ç²å¾—æœ€å¤§è¦†è“‹ç‡æå‡
        
        Args:
            content: å®Œæ•´å…§å®¹
            target_length: ç›®æ¨™æ¡æ¨£é•·åº¦ï¼ˆè‹¥ç‚º None å‰‡å‹•æ…‹è¨ˆç®—ï¼‰
        
        Returns:
            æ¡æ¨£å¾Œçš„å…§å®¹
        """
        total_len = len(content)
        
        # å‹•æ…‹è¨ˆç®—æ¡æ¨£é•·åº¦
        if target_length is None:
            target_length = self._get_dynamic_sample_length(total_len)
        
        # å…§å®¹ä¸é•·ï¼Œç›´æ¥è¿”å›
        if total_len <= target_length:
            return content
        
        # è¨ˆç®—å„éƒ¨åˆ†é•·åº¦ï¼ˆé–‹é ­50% + ä¸­é–“30% + çµå°¾20%ï¼‰
        head_len = int(target_length * 0.5)
        middle_len = int(target_length * 0.3)
        tail_len = int(target_length * 0.2)
        
        # æå–é–‹é ­
        head = content[:head_len]
        
        # æå–ä¸­é–“ï¼ˆå¾ 40% ä½ç½®é–‹å§‹ï¼‰
        middle_start = int(total_len * 0.4)
        middle = content[middle_start:middle_start + middle_len]
        
        # æå–çµå°¾
        tail = content[-tail_len:]
        
        # çµ„åˆï¼ˆæ·»åŠ åˆ†éš”ç¬¦ï¼‰
        sampled = f"{head}\n\n[...ä¸­é–“å…§å®¹çœç•¥...]\n\n{middle}\n\n[...å¾ŒçºŒå…§å®¹çœç•¥...]\n\n{tail}"
        
        print(f"   ğŸ“Š æ™ºæ…§æ¡æ¨£: {total_len} â†’ {len(sampled)} å­— (é–‹é ­+ä¸­æ®µ+çµå°¾)")
        
        return sampled
    
    def _get_dynamic_sample_length(self, content_length: int) -> int:
        """
        å‹•æ…‹æ¡æ¨£é•·åº¦ (YouTube / å°ç´…æ›¸é€å­—ç¨¿)
        
        Token é ç®—åˆ†æ (Cerebras 131K tokens):
        - ç³»çµ± Prompt: ~1K tokens
        - è¼¸å‡ºé ç•™: ~15K tokens (å«æ ¼å¼åŒ–é€å­—ç¨¿)
        - å¯ç”¨è¼¸å…¥: ~115K tokens â‰ˆ 170K å­—
        
        YouTube/å°ç´…æ›¸é€å­—ç¨¿ç‰¹æ€§ï¼š
        - éœ€è¦æ›´å¤šæ¡æ¨£ä»¥ç¢ºä¿å®Œæ•´çŸ¥è­˜æå–
        - åŒ…å«æ ¼å¼åŒ–è¼¸å‡ºéœ€é ç•™æ›´å¤šè¼¸å‡º token
        
        Returns:
            å»ºè­°æ¡æ¨£é•·åº¦
        """
        if content_length > 100000:
            return 15000  # è¶…é•·é€å­—ç¨¿ (>100K) - ç´„ 2-3 å°æ™‚å½±ç‰‡
        elif content_length > 50000:
            return 12000  # é•·é€å­—ç¨¿ (50K-100K) - ç´„ 1-2 å°æ™‚å½±ç‰‡
        elif content_length > 20000:
            return 10000  # ä¸­é€å­—ç¨¿ (20K-50K) - ç´„ 30-60 åˆ†é˜å½±ç‰‡
        else:
            return 6000   # çŸ­é€å­—ç¨¿ (<20K) - ç´„ <30 åˆ†é˜å½±ç‰‡

    
    def identify_speakers(self, transcript: str, video_info: Dict = None) -> str:
        """
        è­˜åˆ¥è¬›è€…ï¼ˆä½¿ç”¨å½±ç‰‡å…ƒæ•¸æ“šè¼”åŠ©è­˜åˆ¥ï¼‰
        
        Args:
            transcript: åŸå§‹é€å­—ç¨¿
            video_info: å½±ç‰‡è³‡è¨Š {'title', 'channel', 'description'}
            
        Returns:
            æ¨™è¨˜è¬›è€…å¾Œçš„é€å­—ç¨¿
        """
        # å¾å½±ç‰‡å…ƒæ•¸æ“šæå–è¬›è€…è³‡è¨Š
        speaker_hints = ""
        if video_info:
            channel = video_info.get('channel', '')
            title = video_info.get('title', '')
            description = video_info.get('description', '')[:500] if video_info.get('description') else ''
            
            speaker_hints = f"""
## å·²çŸ¥è¬›è€…è³‡è¨Šï¼ˆè«‹å„ªå…ˆä½¿ç”¨ï¼‰

- **é »é“ä¸»æŒäºº/ä¸»è¬›è€…**: {channel}
- **å½±ç‰‡æ¨™é¡Œ**: {title}
- **æè¿°æ‘˜è¦**: {description[:200] if description else 'ç„¡'}

### è­˜åˆ¥è¦å‰‡
1. è‹¥ç‚ºå–®äººå½±ç‰‡ï¼ˆVlogã€æ•™å­¸ï¼‰ï¼Œä¸»è¬›è€…ç‚ºé »é“æ“æœ‰è€…ã€Œ{channel}ã€
2. è‹¥ç‚ºè¨ªè«‡ï¼Œä¸»æŒäººé€šå¸¸æ˜¯é »é“æ“æœ‰è€…ã€Œ{channel}ã€
3. è¨ªè«‡å˜‰è³“å§“åå¯èƒ½å‡ºç¾åœ¨æ¨™é¡Œæˆ–æè¿°ä¸­
4. **ç¦æ­¢ä½¿ç”¨è™›æ§‹æˆ–ä½”ä½ç¬¦å§“å**ï¼ˆå¦‚ Cortexã€å¼µä¸‰ç­‰ï¼‰
5. ç„¡æ³•è­˜åˆ¥æ™‚ç”¨ã€Œä¸»è¬›è€…ã€æˆ–ã€Œå˜‰è³“ã€ä»£æ›¿
"""

        prompt = f"""
{self.speaker_prompt}

{speaker_hints}

## å¾…åˆ†æé€å­—ç¨¿

{transcript[:8000]}
"""
        
        result = self.llm.generate(
            prompt=prompt,
            system_prompt=f"ä½ æ˜¯å°ˆæ¥­çš„èªéŸ³åˆ†æå¸«ã€‚æ­¤å½±ç‰‡ä¾†è‡ªé »é“ã€Œ{video_info.get('channel', 'æœªçŸ¥')}ã€ï¼Œè«‹è­˜åˆ¥å°è©±ä¸­çš„ä¸åŒè¬›è€…ã€‚",
            max_tokens=8000,
            temperature=0.3
        )
        
        return result if result else transcript
    
    def _load_ontology_entities(self) -> List[str]:
        """è¼‰å…¥æœ¬é«”è«–å¯¦é«”æ¸…å–® (80/20 å„ªåŒ–)"""
        ontology_path = Path.home() / "R2R/config/ontology/solo_entrepreneur_synonyms.json"
        try:
            if ontology_path.exists():
                import json
                with open(ontology_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                # æå–æ‰€æœ‰ entity çš„ key (ä¸»å¯¦é«”åç¨±)
                return list(data.keys())
        except Exception:
            pass
        return []
    
    def _load_ontology_tags(self) -> List[str]:
        """è¼‰å…¥é è¨­æ¨™ç±¤æ¸…å–® (80/20 å„ªåŒ– - åš´æ ¼é™åˆ¶)"""
        tags_path = Path.home() / "R2R/config/ontology/solo_entrepreneur_tags.yaml"
        try:
            if tags_path.exists():
                import yaml
                with open(tags_path, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f)
                # æå–æ‰€æœ‰ dimensions ä¸‹çš„ tags
                all_tags = []
                for dim_key, dim_val in data.get('dimensions', {}).items():
                    for cat_key, cat_val in dim_val.get('categories', {}).items():
                        all_tags.extend(cat_val.get('tags', []))
                return all_tags
        except Exception:
            pass
        return []
    
    def _fuzzy_match(self, text: str, candidates: List[str], threshold: float = 0.3) -> Optional[str]:
        """
        æ¨¡ç³ŠåŒ¹é…ï¼šæ‰¾åˆ°æœ€æ¥è¿‘çš„å€™é¸é …
        ä½¿ç”¨ç°¡å–®çš„å­—ç¬¦é‡ç–Šç®—æ³•ï¼ˆç„¡éœ€é¡å¤–ä¾è³´ï¼‰
        """
        if not text or not candidates:
            return None
        
        text_lower = text.lower().replace(' ', '')
        best_match = None
        best_score = threshold
        
        for candidate in candidates:
            candidate_lower = candidate.lower().replace(' ', '')
            
            # å®Œå…¨åŒ¹é…
            if text_lower == candidate_lower:
                return candidate
            
            # åŒ…å«åŒ¹é…ï¼ˆå„ªå…ˆï¼‰
            if text_lower in candidate_lower or candidate_lower in text_lower:
                score = min(len(text_lower), len(candidate_lower)) / max(len(text_lower), len(candidate_lower))
                if score > best_score:
                    best_score = score
                    best_match = candidate
                continue
            
            # å­—ç¬¦é‡ç–Šåˆ†æ•¸
            common = set(text_lower) & set(candidate_lower)
            score = len(common) / max(len(set(text_lower)), len(set(candidate_lower)))
            
            if score > best_score:
                best_score = score
                best_match = candidate
        
        return best_match
    
    def _validate_entities(self, entities: List[str]) -> List[str]:
        """
        å¾Œè™•ç†é©—è­‰ï¼šç¢ºä¿ entities 100% ç¬¦åˆé è¨­æœ¬é«”è«–
        ä¸ç¬¦åˆçš„é …ç›®æœƒæ˜ å°„åˆ°æœ€æ¥è¿‘çš„é è¨­å¯¦é«”
        """
        ontology_entities = self._load_ontology_entities()
        if not ontology_entities:
            return entities[:8]  # ç„¡ ontology å‰‡ç›´æ¥è¿”å›
        
        validated = []
        ontology_set = set(ontology_entities)
        
        for entity in entities:
            # å®Œå…¨åŒ¹é…
            if entity in ontology_set:
                if entity not in validated:
                    validated.append(entity)
            else:
                # æ¨¡ç³ŠåŒ¹é…
                match = self._fuzzy_match(entity, ontology_entities)
                if match and match not in validated:
                    validated.append(match)
                    print(f"   ğŸ“ Entity æ˜ å°„: {entity} â†’ {match}")
        
        return validated[:8]  # æœ€å¤š 8 å€‹
    
    def _validate_tags(self, tags: List[str]) -> List[str]:
        """
        å¾Œè™•ç†é©—è­‰ï¼šç¢ºä¿ tags 100% ç¬¦åˆé è¨­æ¨™ç±¤é›†
        ä¸ç¬¦åˆçš„é …ç›®æœƒæ˜ å°„åˆ°æœ€æ¥è¿‘çš„é è¨­æ¨™ç±¤
        """
        ontology_tags = self._load_ontology_tags()
        if not ontology_tags:
            return tags[:5]  # ç„¡é è¨­å‰‡ç›´æ¥è¿”å›
        
        validated = []
        tags_set = set(ontology_tags)
        
        for tag in tags:
            # å®Œå…¨åŒ¹é…
            if tag in tags_set:
                if tag not in validated:
                    validated.append(tag)
            else:
                # æ¨¡ç³ŠåŒ¹é…
                match = self._fuzzy_match(tag, ontology_tags)
                if match and match not in validated:
                    validated.append(match)
                    print(f"   ğŸ·ï¸ Tag æ˜ å°„: {tag} â†’ {match}")
        
        return validated[:5]  # æœ€å¤š 5 å€‹
    
    def _is_interview_content(self, transcript: str, video_info: Dict = None) -> bool:
        """
        é æª¢ï¼šåˆ¤æ–·æ˜¯å¦ç‚ºè¨ªè«‡å…§å®¹ (ä¸èª¿ç”¨ LLMï¼Œç¯€ç´„ API)
        
        æª¢é©—æ©Ÿåˆ¶ï¼š
        1. æ¨™é¡Œé—œéµå­—æª¢æŸ¥
        2. é€å­—ç¨¿å…§å®¹é—œéµå­—æª¢æŸ¥
        
        Returns:
            True = å¯èƒ½æ˜¯è¨ªè«‡ï¼Œéœ€è¦è­˜åˆ¥ guest
            False = éè¨ªè«‡ï¼Œè·³é guest è­˜åˆ¥
        """
        # è¨ªè«‡ç›¸é—œé—œéµå­—
        interview_keywords = [
            'è¨ªè«‡', 'å°ˆè¨ª', 'å°è«‡', 'å°è©±', 'è¨ªå•', 'è«‹åˆ°', 'é‚€è«‹',
            'å˜‰è³“', 'è€å¸«', 'ä¾†è³“', 'ç‰¹åˆ¥å˜‰è³“',
            'interview', 'podcast', 'guest', 'feat', 'ft.', 'with',
            'q&a', 'qa', 'å•ç­”', 'é€£ç·š'
        ]
        
        # 1. æª¢æŸ¥æ¨™é¡Œ
        title = ""
        if video_info:
            title = video_info.get('title', '').lower()
        
        for keyword in interview_keywords:
            if keyword.lower() in title:
                return True
        
        # 2. æª¢æŸ¥é€å­—ç¨¿å‰ 500 å­—
        transcript_head = transcript[:500].lower() if transcript else ""
        
        # è¨ªè«‡é–‹å ´ç‰¹å¾µ
        interview_patterns = [
            'ä»Šå¤©æˆ‘å€‘è«‹åˆ°', 'ä»Šå¤©çš„å˜‰è³“', 'ä»Šå¤©é‚€è«‹', 'æ­¡è¿ä¾†åˆ°',
            'è«‹å•', 'å¯ä»¥ä»‹ç´¹ä¸€ä¸‹', 'å…ˆåšå€‹è‡ªæˆ‘ä»‹ç´¹',
            'è¬è¬é‚€è«‹', 'å¾ˆé«˜èˆˆä¾†åˆ°', 'æ„Ÿè¬ä¸»æŒäºº',
            "today's guest", "welcome to the show", "thanks for having me"
        ]
        
        for pattern in interview_patterns:
            if pattern.lower() in transcript_head:
                return True
        
        return False
    
    def extract_knowledge(self, transcript: str, video_info: Dict = None) -> Dict:
        """
        æå–å•†æ¥­çŸ¥è­˜ï¼ˆåˆä½µèª¿ç”¨ï¼šçŸ¥è­˜ + æ‘˜è¦ + é—œéµå­— + å¯¦é«” + æ¨™ç±¤ + å˜‰è³“ + é€å­—ç¨¿æ ¼å¼åŒ–ï¼‰
        
        80/20 å„ªåŒ–ï¼šåœ¨æºé ­ä¸€æ¬¡å®Œæˆæ‰€æœ‰æå–ï¼Œé¿å… R2R Phase1 é‡è¤‡ API èª¿ç”¨
        æ–°å¢ï¼šé€å­—ç¨¿æ¨™é»ç¬¦è™Ÿèˆ‡æ–·å¥ä¿®å¾©ï¼ˆåŒä¸€èª¿ç”¨ä¸­å®Œæˆï¼‰
        
        Args:
            transcript: é€å­—ç¨¿ (å·²æ¨™è¨˜è¬›è€…)
            video_info: å½±ç‰‡è³‡è¨Š {'title': ..., 'url': ..., 'duration': ...}
            
        Returns:
            æå–çš„çŸ¥è­˜ {'summary': ..., 'knowledge': ..., 'keywords': ..., 'entities': ..., 'tags': ..., 'guest': ..., 'formatted_transcript': ...}
        """
        # æ™ºæ…§æ¡æ¨£å„ªåŒ–ï¼šç§»é™¤é‡è¤‡è¡Œå¾Œä½¿ç”¨å‹•æ…‹æ™ºæ…§æ¡æ¨£
        lines = transcript.split('\n')
        unique_lines = list(dict.fromkeys(lines))
        full_transcript = '\n'.join([l for l in unique_lines if len(l.strip()) > 5])
        clean_transcript = self._smart_sample(full_transcript)  # å‹•æ…‹æ¡æ¨£ (6K-15K)
        
        # è¼‰å…¥æœ¬é«”è«–å¯¦é«” (80/20 å„ªåŒ–)
        ontology_entities = self._load_ontology_entities()
        ontology_tags = self._load_ontology_tags()
        
        ontology_hint = ""
        if ontology_entities:
            ontology_hint = f"""
### å¯¦é«” (Entities) [å¿…å¡« - åš´æ ¼é™åˆ¶]
**è«‹ã€Œåªèƒ½ã€å¾ä»¥ä¸‹é å®šç¾©å¯¦é«”ä¸­é¸æ“‡ 3-8 å€‹æœ€åŒ¹é…çš„ï¼Œç¦æ­¢è‡ªè¡Œå‰µé€ æ–°å¯¦é«”ï¼š**
{', '.join(ontology_entities[:100])}

âš ï¸ æ³¨æ„ï¼šåªèƒ½é¸æ“‡ä¸Šè¿°å¯¦é«”ï¼Œä¸å¾—å‰µé€ ä»»ä½•æ–°é …ç›®ï¼

**å¿…é ˆ**åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- ENTITIES: ["å¯¦é«”1", "å¯¦é«”2", ...] -->`

### æ¨™ç±¤ (Tags) [å¿…å¡« - åš´æ ¼é™åˆ¶]
**è«‹ã€Œåªèƒ½ã€å¾ä»¥ä¸‹é å®šç¾©æ¨™ç±¤ä¸­é¸æ“‡ 3-5 å€‹æœ€åŒ¹é…çš„ï¼Œç¦æ­¢è‡ªè¡Œå‰µé€ æ–°æ¨™ç±¤ï¼š**
{', '.join(ontology_tags[:60])}

âš ï¸ æ³¨æ„ï¼šåªèƒ½é¸æ“‡ä¸Šè¿°æ¨™ç±¤ï¼Œä¸å¾—å‰µé€ ä»»ä½•æ–°é …ç›®ï¼

**å¿…é ˆ**åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- TAGS: ["æ¨™ç±¤1", "æ¨™ç±¤2", ...] -->`
"""
        else:
            # ç„¡ ontology æ™‚çš„å‚™ç”¨ï¼ˆä½†ä»æä¾›å¸¸è¦‹é¸é …ï¼‰
            ontology_hint = """
### å¯¦é«” (Entities) [å¿…å¡«]
è«‹å¾ä»¥ä¸‹å¸¸è¦‹å•†æ¥­æ¦‚å¿µä¸­é¸æ“‡ 3-8 å€‹ï¼š
å•†æ¥­æ¨¡å¼, å‰µæ¥­, ç”¢å“å¸‚å ´åŒ¹é…, å®šä½ç­–ç•¥, è¨‚é–±åˆ¶, å…§å®¹è¡ŒéŠ·, å€‹äººå“ç‰Œ, ç²¾å¯¦å‰µæ¥­, MVP, ç²åˆ©æ¨¡å¼

**å¿…é ˆ**åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- ENTITIES: ["å¯¦é«”1", "å¯¦é«”2", ...] -->`

### æ¨™ç±¤ (Tags) [å¿…å¡«]
è«‹å¾ä»¥ä¸‹å¸¸è¦‹æ¨™ç±¤ä¸­é¸æ“‡ 3-5 å€‹ï¼š
å¸‚å ´å®šä½, åƒ¹å€¼ä¸»å¼µ, è¨‚é–±åˆ¶, å…§å®¹è¡ŒéŠ·, å¾é›¶é–‹å§‹, è¦æ¨¡åŒ–, è‡ªå‹•åŒ–, è¢«å‹•æ”¶å…¥, AIå·¥å…·

**å¿…é ˆ**åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- TAGS: ["æ¨™ç±¤1", "æ¨™ç±¤2", ...] -->`
"""
        
        # è¨ªè«‡å˜‰è³“è­˜åˆ¥ (é æª¢æ©Ÿåˆ¶ç¯€ç´„ API)
        is_interview = self._is_interview_content(clean_transcript, video_info)
        guest_hint = ""
        if is_interview:
            guest_hint = """
### è¨ªè«‡å˜‰è³“ (Guest)
å¦‚æœé€™æ˜¯è¨ªè«‡/å°è«‡å…§å®¹ï¼Œè«‹è­˜åˆ¥å—è¨ªè€…/å˜‰è³“å§“åã€‚
æ³¨æ„ï¼šä¸»æŒäºº/é »é“ä¸»ä¸ç®—å˜‰è³“ï¼Œåªè­˜åˆ¥è¢«é‚€è«‹çš„ä¾†è³“ã€‚
å¦‚ç„¡å˜‰è³“æˆ–ç„¡æ³•è­˜åˆ¥ï¼Œè«‹è¼¸å‡ºç©ºå­—ä¸²ã€‚
è«‹åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- GUEST: "å˜‰è³“å§“å" -->`
"""
        
        # æº–å‚™ä¸Šä¸‹æ–‡
        context = ""
        channel = video_info.get('channel', 'æœªçŸ¥') if video_info else 'æœªçŸ¥'
        if video_info:
            context = f"""
## å½±ç‰‡è³‡è¨Š
- æ¨™é¡Œ: {video_info.get('title', 'æœªçŸ¥')}
- ä¾†æº: {channel}
- æ™‚é•·: {video_info.get('duration', 'æœªçŸ¥')}
"""
        
        # åˆä½µ Promptï¼šçŸ¥è­˜æå– + æ‘˜è¦ + é—œéµå­— + å¯¦é«” + æ¨™ç±¤ + å˜‰è³“ + é€å­—ç¨¿æ ¼å¼åŒ–
        prompt = f"""
{self.knowledge_prompt}

{context}

## é€å­—ç¨¿å…§å®¹

{clean_transcript}

---

## é¡å¤–è¼¸å‡ºï¼ˆè«‹åœ¨çŸ¥è­˜æå–å¾Œæ·»åŠ ï¼Œæ‰€æœ‰æ¨™è¨˜éƒ½æ˜¯å¿…å¡«ï¼‰

### é€å­—ç¨¿æ ¼å¼åŒ–èˆ‡ç¿»è­¯ [å¿…å¡«]
è«‹å°‡é€å­—ç¨¿ç¿»è­¯ç‚º**ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰**ï¼Œä¸¦ä¾ä»¥ä¸‹è¦å‰‡æ•´ç†æ ¼å¼ï¼š

**ç¿»è­¯åŸå‰‡ï¼š**
- å¿ å¯¦å‚³é”åŸæ„ï¼Œä¸å¢åˆªå…§å®¹ï¼Œä¸éåº¦è©®é‡‹
- æ‰€æœ‰èªè¨€ï¼ˆè‹±æ–‡ã€ç°¡é«”ä¸­æ–‡ã€æ—¥æ–‡ç­‰ï¼‰ä¸€å¾‹ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡
- ä½¿ç”¨å°ç£æ…£ç”¨è©å½™ï¼ˆvideoâ†’å½±ç‰‡ã€informationâ†’è³‡è¨Šã€softwareâ†’è»Ÿé«”ã€networkâ†’ç¶²è·¯ã€userâ†’ä½¿ç”¨è€…ï¼‰
- ä¿ç•™å°ˆæœ‰åè©åŸæ–‡ï¼ˆäººåã€å…¬å¸åå¯é™„è‹±æ–‡ï¼Œå¦‚ï¼šä¼Šéš†Â·é¦¬æ–¯å…‹ Elon Muskï¼‰

**æ ¼å¼è¦å‰‡ï¼š**
- æ·»åŠ é©ç•¶æ¨™é»ç¬¦è™Ÿï¼šå¥è™Ÿã€é€—è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿ
- æ¯ 2-4 å¥è©±ç‚ºä¸€æ®µè½ï¼Œä¸»é¡Œè½‰æ›æ™‚æ›è¡Œ
- ä¿ç•™å£èªç‰¹è‰²ï¼ˆå¦‚ã€Œå—¯ã€ã€Œå°ã€ã€Œå°±æ˜¯èªªã€ç­‰èªæ°£è©ï¼‰
- é•·å¥é©ç•¶æ‹†åˆ†ï¼Œç¢ºä¿é–±è®€æµæš¢

**å¿…é ˆ**åœ¨æ–‡æœ«æ·»åŠ ï¼ˆç¿»è­¯ä¸¦æ•´ç†å¾Œçš„é€å­—ç¨¿ï¼‰ï¼š
`<!-- FORMATTED_TRANSCRIPT_START -->`
[ç¿»è­¯ä¸¦æ•´ç†å¾Œçš„ç¹é«”ä¸­æ–‡é€å­—ç¨¿å…¨æ–‡]
`<!-- FORMATTED_TRANSCRIPT_END -->`

### ä¸€å¥è©±æ‘˜è¦ [å¿…å¡«]
**å¿…é ˆ**åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- SUMMARY: [ä¸è¶…é100å­—çš„æ ¸å¿ƒè§€é»æ‘˜è¦] -->`

### é—œéµå­— [å¿…å¡«]
**å¿…é ˆ**åœ¨æ–‡æœ«æ·»åŠ ï¼š
`<!-- KEYWORDS: ["é—œéµå­—1", "é—œéµå­—2", ...] -->`

### è¬›è€…æ¨™è¨˜ [å¿…å¡«]
åœ¨æ ¼å¼åŒ–é€å­—ç¨¿ä¸­ï¼Œè«‹æ¨™è¨˜è¬›è€…èº«ä»½ï¼š
- è‹¥ç‚ºå–®äººå½±ç‰‡ï¼Œä½¿ç”¨ã€Œ**ä¸»è¬›è€… [{channel}]:**ã€
- è‹¥ç‚ºè¨ªè«‡ï¼Œä¸»æŒäººæ¨™è¨˜ç‚ºã€Œ**ä¸»æŒäºº:**ã€ï¼Œå˜‰è³“æ¨™è¨˜ç‚ºã€Œ**å—è¨ªè€… [å§“å]:**ã€
- ç„¡æ³•è­˜åˆ¥è¬›è€…æ™‚ä½¿ç”¨ã€Œ**ä¸»è¬›è€…:**ã€

{ontology_hint}
{guest_hint}
"""
        # å‹•æ…‹èª¿æ•´ max_tokensï¼šé•·é€å­—ç¨¿éœ€è¦æ›´å¤šè¼¸å‡ºç©ºé–“
        transcript_length = len(clean_transcript)
        if transcript_length > 20000:  # è¶…é 2 è¬å­—å…ƒ (ç´„ 60 åˆ†é˜å½±ç‰‡)
            max_tokens = 15000
        elif transcript_length > 10000:  # è¶…é 1 è¬å­—å…ƒ (ç´„ 30 åˆ†é˜å½±ç‰‡)
            max_tokens = 12000
        else:
            max_tokens = 8000
        
        result_text = self.llm.generate(
            prompt=prompt,
            system_prompt="""ä½ æ˜¯è³‡æ·±é€å­—ç¨¿è™•ç†å°ˆå®¶åŠç¿»è­¯å¤§å¸«ï¼ŒåŒæ™‚ä¹Ÿæ˜¯å•†æ¥­çŸ¥è­˜æå–å°ˆå®¶ã€‚

ã€é€å­—ç¨¿è™•ç†åŸå‰‡ã€‘
1. å¿ å¯¦åŸæ„ï¼šç¿»è­¯æ™‚å¿…é ˆä¿ç•™è¬›è€…åŸæ„ï¼Œä¸å¢åˆªã€ä¸æ”¹å¯«ã€ä¸éåº¦è©®é‡‹
2. é©ç•¶æ–·å¥ï¼šä¾èªæ„è‡ªç„¶åœé “è™•åŠ å…¥æ¨™é»ç¬¦è™Ÿï¼ˆå¥è™Ÿã€é€—è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿï¼‰
3. æ®µè½åˆ†æ˜ï¼šæ¯ 2-4 å¥è©±ç‚ºä¸€æ®µï¼Œä¸»é¡Œè½‰æ›æ™‚æ›è¡Œåˆ†æ®µ
4. å£èªä¿ç•™ï¼šä¿ç•™è¬›è€…çš„å£èªç‰¹è‰²å’Œèªæ°£è©ï¼ˆå¦‚ã€Œå—¯ã€ã€Œå°ã€ã€Œå°±æ˜¯èªªã€ç­‰ï¼‰
5. å°ˆæœ‰åè©ï¼šäººåã€å…¬å¸åã€ç”¢å“åä¿ç•™åŸæ–‡æˆ–é™„è¨»è‹±æ–‡
6. å°ç£ç”¨èªï¼šä½¿ç”¨ç¹é«”ä¸­æ–‡åŠå°ç£æ…£ç”¨è©ï¼ˆå½±ç‰‡ã€è³‡è¨Šã€è»Ÿé«”ã€ç¶²è·¯ã€ä½¿ç”¨è€…ï¼‰

ã€è¼¸å‡ºè¦æ±‚ã€‘
è«‹åœ¨æ–‡æœ«æŒ‰æŒ‡å®šæ ¼å¼æ·»åŠ æ‰€æœ‰å¿…å¡«æ¨™è¨˜ï¼ˆæ‘˜è¦ã€é—œéµå­—ã€å¯¦é«”ã€æ¨™ç±¤ã€æ ¼å¼åŒ–é€å­—ç¨¿ï¼‰ã€‚æ¯å€‹æ¨™è¨˜éƒ½å¿…é ˆè¼¸å‡ºã€‚""",
            max_tokens=max_tokens,
            temperature=0.3  # é™ä½æº«åº¦ä»¥æé«˜ç¿»è­¯ç©©å®šæ€§
        )
        
        if not result_text:
            return {"error": "çŸ¥è­˜æå–å¤±æ•—"}
        
        # è§£æåˆä½µçµæœ
        summary = ""
        keywords = []
        entities = []
        tags = []
        knowledge = result_text
        
        # æå–æ‘˜è¦
        import re
        import json
        
        summary_match = re.search(r'<!-- SUMMARY: (.+?) -->', result_text)
        if summary_match:
            summary = summary_match.group(1).strip()
            knowledge = knowledge.replace(summary_match.group(0), '')
        
        # æå–é—œéµå­—
        keywords_match = re.search(r'<!-- KEYWORDS: (\[.+?\]) -->', result_text)
        if keywords_match:
            try:
                keywords = json.loads(keywords_match.group(1))
                knowledge = knowledge.replace(keywords_match.group(0), '')
            except:
                pass
        
        # æå–å¯¦é«” (80/20 å„ªåŒ–)
        entities_match = re.search(r'<!-- ENTITIES: (\[.+?\]) -->', result_text)
        if entities_match:
            try:
                entities = json.loads(entities_match.group(1))
                knowledge = knowledge.replace(entities_match.group(0), '')
            except:
                pass
        
        # æå–æ¨™ç±¤ (80/20 å„ªåŒ–)
        tags_match = re.search(r'<!-- TAGS: (\[.+?\]) -->', result_text)
        if tags_match:
            try:
                tags = json.loads(tags_match.group(1))
                knowledge = knowledge.replace(tags_match.group(0), '')
            except:
                pass
        
        # æå–è¨ªè«‡å˜‰è³“ (æ¢ä»¶å¼ï¼šåªåœ¨è¨ªè«‡å…§å®¹æ™‚æå–)
        guest = None
        guest_match = re.search(r'<!-- GUEST: "(.+?)" -->', result_text)
        if guest_match:
            guest = guest_match.group(1).strip()
            if guest and guest not in ['', 'ç„¡', 'None', 'null', 'ç„¡æ³•è­˜åˆ¥']:
                knowledge = knowledge.replace(guest_match.group(0), '')
            else:
                guest = None
        
        # æå–æ ¼å¼åŒ–é€å­—ç¨¿ (80/20 å„ªåŒ–ï¼šå–®æ¬¡ API å®Œæˆæ¨™é»ç¬¦è™Ÿä¿®å¾©)
        formatted_transcript = None
        transcript_match = re.search(
            r'<!-- FORMATTED_TRANSCRIPT_START -->\s*(.*?)\s*<!-- FORMATTED_TRANSCRIPT_END -->', 
            result_text, 
            re.DOTALL
        )
        if transcript_match:
            formatted_transcript = transcript_match.group(1).strip()
            knowledge = knowledge.replace(transcript_match.group(0), '')
        
        # å¾Œè™•ç†é©—è­‰ï¼šç¢ºä¿ entities å’Œ tags 100% ç¬¦åˆé è¨­æœ¬é«”è«–
        validated_entities = self._validate_entities(entities) if entities else []
        validated_tags = self._validate_tags(tags) if tags else []
        
        return {
            "knowledge": knowledge.strip(),
            "summary": summary,
            "keywords": keywords,
            "entities": validated_entities,  # é©—è­‰å¾Œçš„ entities
            "tags": validated_tags,  # é©—è­‰å¾Œçš„ tags
            "guest": guest,
            "formatted_transcript": formatted_transcript,
            "metadata": {
                "processed_at": datetime.now().isoformat(),
                "llm_provider": self.llm.current_provider,
                "video_info": video_info,
                "optimized": True,
                "ontology_used": len(ontology_entities) > 0,
                "is_interview": is_interview,
                "entities_validated": len(validated_entities) > 0,
                "tags_validated": len(validated_tags) > 0
            }
        }
    def _should_skip_speaker_id(self, video_info: Dict) -> bool:
        """
        åˆ¤æ–·æ˜¯å¦è·³éè¬›è€…è­˜åˆ¥ï¼ˆå„ªåŒ– API èª¿ç”¨ï¼‰
        
        è·³éæ¢ä»¶ï¼š
        - æ¨™é¡Œä¸åŒ…å«è¨ªè«‡ç›¸é—œè©å½™
        - éæ˜é¡¯å¤šäººå°è©±å…§å®¹
        """
        if not video_info:
            return False
        
        title = video_info.get('title', '').lower()
        
        # è¨ªè«‡ç›¸é—œé—œéµå­—ï¼ˆéœ€è¦è¬›è€…è­˜åˆ¥ï¼‰
        interview_keywords = [
            'è¨ªè«‡', 'å°ˆè¨ª', 'å°è«‡', 'å°è©±', 'interview', 'podcast', 
            'å˜‰è³“', 'guest', 'feat', 'ft.', 'ft', 'with', 'èˆ‡', 'å’Œ',
            'q&a', 'qa', 'å•ç­”'
        ]
        
        # å¦‚æœæ¨™é¡ŒåŒ…å«è¨ªè«‡é—œéµå­—ï¼Œä¸è·³é
        for keyword in interview_keywords:
            if keyword in title:
                return False
        
        # å–®äººå…§å®¹é—œéµå­—ï¼ˆå¯è·³éè¬›è€…è­˜åˆ¥ï¼‰
        solo_keywords = [
            'vlog', 'æ•™å­¸', 'tutorial', 'guide', 'åˆ†äº«', 'å¿ƒå¾—',
            'review', 'è©•æ¸¬', 'é–‹ç®±', 'unbox', 'æ—¥å¸¸', 'routine'
        ]
        
        for keyword in solo_keywords:
            if keyword in title:
                return True
        
        # é è¨­ï¼šä¸è·³éï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        return False
    
    def process_transcript(self, transcript: str, video_info: Dict = None) -> Dict:
        """
        å®Œæ•´è™•ç†é€å­—ç¨¿ï¼ˆå„ªåŒ–ç‰ˆ - å–®æ¬¡ LLM èª¿ç”¨ï¼‰
        
        80/20 å„ªåŒ–ï¼šåˆä½µæ‰€æœ‰è™•ç†ç‚ºå–®æ¬¡ API èª¿ç”¨
        - è¬›è€…è­˜åˆ¥ï¼ˆæ•´åˆåˆ° promptï¼‰
        - çŸ¥è­˜æå–
        - æ‘˜è¦/é—œéµå­—/å¯¦é«”/æ¨™ç±¤
        - é€å­—ç¨¿æ ¼å¼åŒ–
        
        Args:
            transcript: åŸå§‹é€å­—ç¨¿
            video_info: å½±ç‰‡è³‡è¨Š
            
        Returns:
            è™•ç†çµæœ
        """
        print("ğŸ” é–‹å§‹è™•ç†é€å­—ç¨¿...")
        print("   ğŸ“š å–®æ¬¡ API èª¿ç”¨ä¸­ï¼ˆè¬›è€…+çŸ¥è­˜+æ ¼å¼åŒ–ï¼‰...")
        
        # ç›´æ¥èª¿ç”¨ extract_knowledgeï¼ˆå·²åŒ…å«è¬›è€…è­˜åˆ¥æŒ‡ä»¤ï¼‰
        result = self.extract_knowledge(transcript, video_info)
        
        # ä½¿ç”¨æ ¼å¼åŒ–å¾Œçš„é€å­—ç¨¿ä½œç‚ºæ¨™è¨˜ç‰ˆæœ¬
        result["marked_transcript"] = result.get('formatted_transcript') or transcript
        
        print("âœ… è™•ç†å®Œæˆ!")
        return result


if __name__ == "__main__":
    print("ğŸ§  MediaMiner Knowledge Extractor")
    print("=" * 50)
    
    extractor = KnowledgeExtractor()
    
    # æ¸¬è©¦æ–‡æœ¬
    test_transcript = """
    ä¸»æŒäººï¼šå¤§å®¶å¥½ï¼Œæ­¡è¿ä¾†åˆ°ä»Šå¤©çš„ç¯€ç›®ã€‚ä»Šå¤©æˆ‘å€‘é‚€è«‹åˆ°äº†çŸ¥åå‰µæ¥­è€…å¼µå…ˆç”Ÿã€‚
    å¼µå…ˆç”Ÿï¼šè¬è¬é‚€è«‹ã€‚
    ä¸»æŒäººï¼šæ‚¨èƒ½è·Ÿæˆ‘å€‘åˆ†äº«ä¸€ä¸‹å‰µæ¥­åˆæœŸæœ€é‡è¦çš„æ˜¯ä»€éº¼å—ï¼Ÿ
    å¼µå…ˆç”Ÿï¼šæˆ‘èªç‚ºæœ€é‡è¦çš„æ˜¯æ‰¾åˆ°ç”¢å“å¸‚å ´åŒ¹é…ã€‚å¾ˆå¤šå‰µæ¥­è€…ä¸€é–‹å§‹å°±æƒ³è‘—æ“´å¼µï¼Œ
    ä½†å…¶å¯¦æ‡‰è©²å…ˆé©—è­‰ä½ çš„ç”¢å“æ˜¯å¦çœŸæ­£è§£æ±ºäº†ç”¨æˆ¶çš„ç—›é»ã€‚
    """
    
    result = extractor.process_transcript(
        test_transcript,
        {"title": "å‰µæ¥­è¨ªè«‡", "channel": "æ¸¬è©¦é »é“"}
    )
    
    print("\nğŸ“Š çµæœ:")
    print(f"æ‘˜è¦: {result.get('summary', 'N/A')}")
    print(f"é—œéµå­—: {result.get('keywords', [])}")
