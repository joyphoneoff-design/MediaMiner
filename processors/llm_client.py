#!/usr/bin/env python3
"""
LLM å®¢æˆ¶ç«¯
æ”¯æŒå¤šæä¾›å•†è‡ªå‹•åˆ‡æ› (80/20 æ³•å‰‡ - å…è²»å„ªå…ˆ)
"""

import os
import time
from pathlib import Path
from typing import Optional, Dict, List, Generator
from dotenv import load_dotenv

# è¼‰å…¥ API å¯†é‘°
env_path = Path(__file__).parent.parent / "config" / "api_keys.env"
load_dotenv(env_path)


class LLMClient:
    """å¤šæä¾›å•† LLM å®¢æˆ¶ç«¯"""
    
    PROVIDERS = [
        {
            "name": "cerebras",
            "priority": 1,
            "model": "qwen-3-235b-a22b-instruct-2507",
            "env_keys": ["CEREBRAS_API_KEY_1", "CEREBRAS_API_KEY_2", "CEREBRAS_API_KEY_3", "CEREBRAS_API_KEY_4"],  # 4 å¸³è™Ÿè¼ªæ›
            "base_url": "https://api.cerebras.ai/v1"
        },
        {
            "name": "gemini",
            "priority": 2,
            "model": "gemini-2.5-flash-lite",
            "env_keys": ["GEMINI_API_KEY_1", "GEMINI_API_KEY_2"],  # 2 å¸³è™Ÿè¼ªæ›
        },
        {
            "name": "openrouter",
            "priority": 3,
            "model": "google/gemini-2.0-flash-exp:free",
            "env_keys": ["OPENROUTER_API_KEY"],
            "base_url": "https://openrouter.ai/api/v1"
        },
        {
            "name": "lmstudio",
            "priority": 4,
            "model": "qwen/qwen3-30b-a3b-2507",
            "base_url": "http://localhost:1234/v1"
        },
        {
            "name": "openai",
            "priority": 5,
            "model": "gpt-4o-mini",
            "env_keys": ["OPENAI_API_KEY"]
        }
    ]
    
    def __init__(self):
        self.current_provider = None
        self.retry_count = 0
        self.max_retries = 3
        # æ™ºèƒ½é™é€Ÿè¿½è¹¤
        self._rate_limit_count = 0
        self._success_count = 0  # é€£çºŒæˆåŠŸè¨ˆæ•¸
        self._last_rate_limit_time = 0
        self._recommended_workers = 10  # é è¨­æœ€å¤§
        self._max_workers = 10  # ä¸Šé™
        self._min_workers = 2   # ä¸‹é™
        
    def record_rate_limit(self):
        """è¨˜éŒ„ 429 é™é€Ÿäº‹ä»¶ (é€æ­¥é™ä½)"""
        import time
        current_time = time.time()
        
        # 1 åˆ†é˜å…§çš„é™é€Ÿäº‹ä»¶æ‰ç´¯è¨ˆ
        if current_time - self._last_rate_limit_time > 60:
            self._rate_limit_count = 0
        
        self._rate_limit_count += 1
        self._success_count = 0  # é‡ç½®æˆåŠŸè¨ˆæ•¸
        self._last_rate_limit_time = current_time
        
        # é€æ­¥é™ä½ (æ¯æ¬¡é™ 1ï¼Œæœ€ä½ 2)
        if self._rate_limit_count >= 2:
            old_workers = self._recommended_workers
            self._recommended_workers = max(self._min_workers, self._recommended_workers - 1)
            if self._recommended_workers < old_workers:
                print(f"   ğŸ”½ é™ä½ä¸¦è¡Œæ•¸: {old_workers} â†’ {self._recommended_workers}")
    
    def record_success(self):
        """è¨˜éŒ„æˆåŠŸäº‹ä»¶ (ç”¨æ–¼è‡ªå‹•æ¢å¾©)"""
        self._success_count += 1
        
        # é€£çºŒæˆåŠŸ 5 æ¬¡ä¸”æœ‰é™é€Ÿè¨˜éŒ„æ™‚ï¼Œå˜—è©¦æ¢å¾©
        if self._success_count >= 5 and self._recommended_workers < self._max_workers:
            old_workers = self._recommended_workers
            self._recommended_workers = min(self._max_workers, self._recommended_workers + 1)
            if self._recommended_workers > old_workers:
                print(f"   ğŸ”¼ æ¢å¾©ä¸¦è¡Œæ•¸: {old_workers} â†’ {self._recommended_workers}")
            self._success_count = 0  # é‡ç½®è¨ˆæ•¸
    
    def get_recommended_workers(self) -> int:
        """ç²å–å»ºè­°çš„ä¸¦è¡Œæ•¸"""
        return self._recommended_workers
    
    def reset_rate_limit_tracking(self):
        """é‡ç½®é™é€Ÿè¿½è¹¤ï¼ˆæ–°æ‰¹æ¬¡é–‹å§‹æ™‚ï¼‰"""
        self._rate_limit_count = 0
        self._success_count = 0
        self._recommended_workers = self._max_workers
    
    def _auto_start_lmstudio(self, model: str) -> bool:
        """è‡ªå‹•å•Ÿå‹• LM Studio ä¼ºæœå™¨ä¸¦è¼‰å…¥æ¨¡å‹"""
        import subprocess
        try:
            # å•Ÿå‹•ä¼ºæœå™¨
            print(f"   ğŸš€ å•Ÿå‹• LM Studio ä¼ºæœå™¨...")
            subprocess.run(["lms", "server", "start"], 
                          capture_output=True, timeout=10)
            time.sleep(2)
            
            # è¼‰å…¥æ¨¡å‹
            print(f"   ğŸ“¥ è¼‰å…¥æ¨¡å‹ {model}...")
            result = subprocess.run(
                ["lms", "load", model, "--yes"],
                capture_output=True, timeout=60
            )
            
            if result.returncode == 0:
                print(f"   âœ… LM Studio å•Ÿå‹•æˆåŠŸ!")
                time.sleep(2)  # ç­‰å¾…æ¨¡å‹å®Œå…¨è¼‰å…¥
                return True
            else:
                print(f"   âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—")
                return False
        except Exception as e:
            print(f"   âŒ LM Studio å•Ÿå‹•å¤±æ•—: {e}")
            return False
    def _get_gemini_client(self, api_key: str):
        """åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯"""
        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            return genai
        except ImportError:
            print("âŒ google-generativeai not installed")
            return None
    
    def _get_openai_compatible_client(self, base_url: str, api_key: str):
        """åˆå§‹åŒ– OpenAI å…¼å®¹å®¢æˆ¶ç«¯"""
        try:
            from openai import OpenAI
            return OpenAI(base_url=base_url, api_key=api_key)
        except ImportError:
            print("âŒ openai not installed")
            return None
    
    def generate(self, prompt: str, system_prompt: str = None, 
                 max_tokens: int = 4096, temperature: float = 0.7) -> Optional[str]:
        """
        ç”Ÿæˆæ–‡æœ¬ (è‡ªå‹•åˆ‡æ›æä¾›å•†)
        
        Args:
            prompt: ç”¨æˆ¶æç¤º
            system_prompt: ç³»çµ±æç¤º
            max_tokens: æœ€å¤§ token æ•¸
            temperature: ç”Ÿæˆæº«åº¦
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        for provider in self.PROVIDERS:
            result = self._try_provider(provider, prompt, system_prompt, 
                                        max_tokens, temperature)
            if result:
                return result
        
        print("âŒ æ‰€æœ‰ LLM æä¾›å•†éƒ½å¤±æ•—")
        return None
    
    def _try_provider(self, provider: Dict, prompt: str, 
                      system_prompt: str, max_tokens: int, 
                      temperature: float) -> Optional[str]:
        """å˜—è©¦å–®å€‹æä¾›å•†ï¼ˆæ”¯æ´å¤šå¸³è™Ÿè¼ªæ›ï¼‰"""
        name = provider["name"]
        model = provider["model"]
        
        # ç²å– API å¯†é‘°åˆ—è¡¨ï¼ˆæ”¯æ´å¤šå¸³è™Ÿè¼ªæ›ï¼‰
        api_keys = []
        if "env_keys" in provider:
            for key_name in provider["env_keys"]:
                key = os.getenv(key_name)
                if key:
                    api_keys.append(key)
        
        if name != "lmstudio" and not api_keys:
            return None
        
        # å˜—è©¦æ¯å€‹ API key
        for key_idx, api_key in enumerate(api_keys if api_keys else [None]):
            key_suffix = f" [å¸³è™Ÿ {key_idx + 1}/{len(api_keys)}]" if len(api_keys) > 1 else ""
            print(f"ğŸ”„ å˜—è©¦ {name} ({model}){key_suffix}...")
            
            try:
                if name == "gemini":
                    result = self._call_gemini(api_key, model, prompt, 
                                            system_prompt, max_tokens, temperature)
                    if result:
                        self.record_success()  # è¨˜éŒ„æˆåŠŸ
                        return result
                elif name == "lmstudio":
                    try:
                        return self._call_openai_compatible(
                            provider.get("base_url", "http://localhost:1234/v1"),
                            "lm-studio", model, prompt, system_prompt, 
                            max_tokens, temperature
                        )
                    except Exception as lm_err:
                        if "Connection" in str(lm_err) or "refused" in str(lm_err).lower():
                            print(f"   ğŸ”§ å˜—è©¦è‡ªå‹•å•Ÿå‹• LM Studio...")
                            if self._auto_start_lmstudio(model):
                                return self._call_openai_compatible(
                                    provider.get("base_url", "http://localhost:1234/v1"),
                                    "lm-studio", model, prompt, system_prompt, 
                                    max_tokens, temperature
                                )
                        raise lm_err
                else:
                    result = self._call_openai_compatible(
                        provider.get("base_url", "https://api.openai.com/v1"),
                        api_key, model, prompt, system_prompt,
                        max_tokens, temperature
                    )
                    if result:
                        self.record_success()  # è¨˜éŒ„æˆåŠŸ
                        return result
            except Exception as e:
                print(f"   âš ï¸ {name} å¤±æ•—: {e}")
                # 429 é™é€Ÿæ™‚è¨˜éŒ„ä¸¦ç­‰å¾…ï¼Œç„¶å¾Œå˜—è©¦ä¸‹ä¸€å€‹ key
                if "429" in str(e) or "rate limit" in str(e).lower() or "quota" in str(e).lower():
                    self.record_rate_limit()
                    # æŒ‡æ•¸é€€é¿å»¶é²: 2, 4, 8 ç§’...
                    delay = min(2 ** self._rate_limit_count, 16)
                    print(f"   â³ ç­‰å¾… {delay} ç§’å¾Œå˜—è©¦ä¸‹ä¸€å€‹å¸³è™Ÿ...")
                    time.sleep(delay)
                    continue  # å˜—è©¦ä¸‹ä¸€å€‹ key
                # å…¶ä»–éŒ¯èª¤ä¹Ÿå˜—è©¦ä¸‹ä¸€å€‹ key
                continue
        
        return None  # æ‰€æœ‰ keys éƒ½å¤±æ•—
    
    def _call_gemini(self, api_key: str, model: str, prompt: str,
                     system_prompt: str, max_tokens: int, 
                     temperature: float) -> Optional[str]:
        """èª¿ç”¨ Gemini API"""
        import google.generativeai as genai
        
        genai.configure(api_key=api_key)
        
        generation_config = {
            "temperature": temperature,
            "max_output_tokens": max_tokens,
        }
        
        model_instance = genai.GenerativeModel(
            model_name=model,
            generation_config=generation_config,
            system_instruction=system_prompt if system_prompt else None
        )
        
        response = model_instance.generate_content(prompt)
        
        if response.text:
            print(f"   âœ… Gemini æˆåŠŸ")
            self.current_provider = "gemini"
            return response.text
        
        return None
    
    def _call_openai_compatible(self, base_url: str, api_key: str, 
                                model: str, prompt: str, system_prompt: str,
                                max_tokens: int, temperature: float) -> Optional[str]:
        """èª¿ç”¨ OpenAI å…¼å®¹ API"""
        from openai import OpenAI
        
        client = OpenAI(base_url=base_url, api_key=api_key)
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        if response.choices and response.choices[0].message.content:
            provider_name = "openai" if "openai.com" in base_url else \
                           "lmstudio" if "localhost" in base_url else \
                           base_url.split("//")[1].split(".")[0]
            print(f"   âœ… {provider_name} æˆåŠŸ")
            self.current_provider = provider_name
            return response.choices[0].message.content
        
        return None


# å–®ä¾‹
_llm_client = None

def get_llm_client() -> LLMClient:
    """ç²å– LLM å®¢æˆ¶ç«¯å–®ä¾‹"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client


if __name__ == "__main__":
    print("ğŸ¤– MediaMiner LLM Client")
    print("=" * 50)
    
    client = get_llm_client()
    
    # æ¸¬è©¦
    response = client.generate(
        prompt="è«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡å–®ä»‹ç´¹ä»€éº¼æ˜¯å•†æ¥­æ¨¡å¼ç•«å¸ƒï¼Ÿ",
        system_prompt="ä½ æ˜¯ä¸€ä½å•†æ¥­é¡§å•ï¼Œè«‹ç”¨ç°¡æ½”çš„èªè¨€å›ç­”ã€‚",
        max_tokens=500
    )
    
    if response:
        print("\nğŸ“ å›æ‡‰:")
        print(response)
