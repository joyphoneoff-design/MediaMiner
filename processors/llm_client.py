#!/usr/bin/env python3
"""
LLM å®¢æˆ¶ç«¯
æ”¯æŒå¤šæä¾›å•†è‡ªå‹•åˆ‡æ› (80/20 æ³•å‰‡ - å…è²»å„ªå…ˆ)
"""

import os
import time
from pathlib import Path
from typing import Optional, Dict, List, Generator
from dotenv import load_dotenv

# è¼‰å…¥ API å¯†é‘°
env_path = Path(__file__).parent.parent / "config" / "api_keys.env"
load_dotenv(env_path)


class LLMClient:
    """å¤šæä¾›å•† LLM å®¢æˆ¶ç«¯"""
    
    PROVIDERS = [
        {
            "name": "cerebras",
            "priority": 1,
            "model": "qwen-3-235b-a22b-instruct-2507",  # Qwen 235B - å…è²»ç©©å®š
            "env_keys": ["CEREBRAS_API_KEY"],
            "base_url": "https://api.cerebras.ai/v1"
        },
        {
            "name": "openrouter",
            "priority": 2,
            "model": "google/gemini-2.0-flash-exp:free",  # 16 req/min é™åˆ¶
            "env_keys": ["OPENROUTER_API_KEY"],
            "base_url": "https://openrouter.ai/api/v1"
        },
        {
            "name": "gemini",
            "priority": 3,
            "model": "gemini-2.5-flash-lite",  # API key éœ€æ›´æ–°
            "env_keys": ["GEMINI_API_KEY", "GEMINI_API_KEY_BACKUP"]
        },
        {
            "name": "cerebras_glm",
            "priority": 4,
            "model": "zai-glm-4.6",  # GLM 4.6 å‚™ç”¨
            "env_keys": ["CEREBRAS_API_KEY"],
            "base_url": "https://api.cerebras.ai/v1"
        },
        {
            "name": "lmstudio",
            "priority": 5,
            "model": "qwen3-30b-a3b",
            "base_url": "http://localhost:1234/v1"
        },
        {
            "name": "openai",
            "priority": 6,
            "model": "gpt-4o-mini",
            "env_keys": ["OPENAI_API_KEY"]
        }
    ]
    
    def __init__(self):
        self.current_provider = None
        self.retry_count = 0
        self.max_retries = 3
        
    def _get_gemini_client(self, api_key: str):
        """åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯"""
        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            return genai
        except ImportError:
            print("âŒ google-generativeai not installed")
            return None
    
    def _get_openai_compatible_client(self, base_url: str, api_key: str):
        """åˆå§‹åŒ– OpenAI å…¼å®¹å®¢æˆ¶ç«¯"""
        try:
            from openai import OpenAI
            return OpenAI(base_url=base_url, api_key=api_key)
        except ImportError:
            print("âŒ openai not installed")
            return None
    
    def generate(self, prompt: str, system_prompt: str = None, 
                 max_tokens: int = 4096, temperature: float = 0.7) -> Optional[str]:
        """
        ç”Ÿæˆæ–‡æœ¬ (è‡ªå‹•åˆ‡æ›æä¾›å•†)
        
        Args:
            prompt: ç”¨æˆ¶æç¤º
            system_prompt: ç³»çµ±æç¤º
            max_tokens: æœ€å¤§ token æ•¸
            temperature: ç”Ÿæˆæº«åº¦
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        for provider in self.PROVIDERS:
            result = self._try_provider(provider, prompt, system_prompt, 
                                        max_tokens, temperature)
            if result:
                return result
        
        print("âŒ æ‰€æœ‰ LLM æä¾›å•†éƒ½å¤±æ•—")
        return None
    
    def _try_provider(self, provider: Dict, prompt: str, 
                      system_prompt: str, max_tokens: int, 
                      temperature: float) -> Optional[str]:
        """å˜—è©¦å–®å€‹æä¾›å•†"""
        name = provider["name"]
        model = provider["model"]
        
        # ç²å– API å¯†é‘°
        api_key = None
        if "env_keys" in provider:
            for key_name in provider["env_keys"]:
                api_key = os.getenv(key_name)
                if api_key:
                    break
        
        if name != "lmstudio" and not api_key:
            return None
        
        print(f"ğŸ”„ å˜—è©¦ {name} ({model})...")
        
        try:
            if name == "gemini":
                return self._call_gemini(api_key, model, prompt, 
                                        system_prompt, max_tokens, temperature)
            elif name == "lmstudio":
                return self._call_openai_compatible(
                    provider.get("base_url", "http://localhost:1234/v1"),
                    "lm-studio", model, prompt, system_prompt, 
                    max_tokens, temperature
                )
            else:
                return self._call_openai_compatible(
                    provider.get("base_url", "https://api.openai.com/v1"),
                    api_key, model, prompt, system_prompt,
                    max_tokens, temperature
                )
        except Exception as e:
            print(f"   âš ï¸ {name} å¤±æ•—: {e}")
            # 429 é™é€Ÿæ™‚ç­‰å¾… 2 ç§’å†å˜—è©¦ä¸‹ä¸€å€‹æä¾›å•†
            if "429" in str(e) or "rate limit" in str(e).lower():
                print(f"   â³ ç­‰å¾… 2 ç§’...")
                time.sleep(2)
            return None
    
    def _call_gemini(self, api_key: str, model: str, prompt: str,
                     system_prompt: str, max_tokens: int, 
                     temperature: float) -> Optional[str]:
        """èª¿ç”¨ Gemini API"""
        import google.generativeai as genai
        
        genai.configure(api_key=api_key)
        
        generation_config = {
            "temperature": temperature,
            "max_output_tokens": max_tokens,
        }
        
        model_instance = genai.GenerativeModel(
            model_name=model,
            generation_config=generation_config,
            system_instruction=system_prompt if system_prompt else None
        )
        
        response = model_instance.generate_content(prompt)
        
        if response.text:
            print(f"   âœ… Gemini æˆåŠŸ")
            self.current_provider = "gemini"
            return response.text
        
        return None
    
    def _call_openai_compatible(self, base_url: str, api_key: str, 
                                model: str, prompt: str, system_prompt: str,
                                max_tokens: int, temperature: float) -> Optional[str]:
        """èª¿ç”¨ OpenAI å…¼å®¹ API"""
        from openai import OpenAI
        
        client = OpenAI(base_url=base_url, api_key=api_key)
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        if response.choices and response.choices[0].message.content:
            provider_name = "openai" if "openai.com" in base_url else \
                           "lmstudio" if "localhost" in base_url else \
                           base_url.split("//")[1].split(".")[0]
            print(f"   âœ… {provider_name} æˆåŠŸ")
            self.current_provider = provider_name
            return response.choices[0].message.content
        
        return None


# å–®ä¾‹
_llm_client = None

def get_llm_client() -> LLMClient:
    """ç²å– LLM å®¢æˆ¶ç«¯å–®ä¾‹"""
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client


if __name__ == "__main__":
    print("ğŸ¤– MediaMiner LLM Client")
    print("=" * 50)
    
    client = get_llm_client()
    
    # æ¸¬è©¦
    response = client.generate(
        prompt="è«‹ç”¨ç¹é«”ä¸­æ–‡ç°¡å–®ä»‹ç´¹ä»€éº¼æ˜¯å•†æ¥­æ¨¡å¼ç•«å¸ƒï¼Ÿ",
        system_prompt="ä½ æ˜¯ä¸€ä½å•†æ¥­é¡§å•ï¼Œè«‹ç”¨ç°¡æ½”çš„èªè¨€å›ç­”ã€‚",
        max_tokens=500
    )
    
    if response:
        print("\nğŸ“ å›æ‡‰:")
        print(response)
