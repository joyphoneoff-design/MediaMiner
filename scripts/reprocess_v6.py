#!/usr/bin/env python3
"""
MediaMiner MD é‡æ–°è™•ç†è…³æœ¬ v6 - é æƒæå»é‡ç‰ˆ
æ ¸å¿ƒæ”¹é€²ï¼š
1. é æƒææ‰€æœ‰æª”æ¡ˆï¼Œå»ºç«‹ hash â†’ ç¬¬ä¸€å€‹æª”æ¡ˆ çš„æ˜ å°„
2. åªæŠŠå”¯ä¸€çš„æª”æ¡ˆé€å…¥è™•ç†ä½‡åˆ—ï¼ˆå¾æºé ­æœçµ•é‡è¤‡ï¼‰
3. å¤šç·šç¨‹è™•ç† API èª¿ç”¨ï¼ˆç„¡ç«¶æ…‹æ¢ä»¶é¢¨éšªï¼‰
"""

import os
import re
import json
import yaml
import sys
import time
import threading
import random
import hashlib
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

print("=" * 70, flush=True)
print("MediaMiner é‡æ–°è™•ç†è…³æœ¬ v6 - é æƒæå»é‡ç‰ˆ", flush=True)
print("=" * 70, flush=True)

# æ·»åŠ  MediaMiner è·¯å¾‘
sys.path.insert(0, str(Path.home() / "MediaMiner"))

# é…ç½®
INPUT_DIR = Path.home() / "Documents/MediaMiner_Data/processed"
OUTPUT_DIR = Path.home() / "Documents/MediaMiner_Data/reprocessed"
PROGRESS_FILE = OUTPUT_DIR / ".progress.json"
MAX_THREADS = 10
MIN_THREADS = 1

# è‡ªé©æ‡‰æ§åˆ¶åƒæ•¸
INITIAL_DELAY = 1.0
MAX_DELAY = 60.0
BACKOFF_FACTOR = 1.5
ERROR_THRESHOLD = 3
SUCCESS_THRESHOLD = 10

# API å¯†é‘°è¼‰å…¥
def load_cerebras_keys():
    keys = []
    config_file = Path.home() / "MediaMiner/config/api_keys.env"
    if config_file.exists():
        with open(config_file, 'r') as f:
            for line in f:
                if line.startswith('CEREBRAS_API_KEY'):
                    parts = line.strip().split('=')
                    if len(parts) == 2 and parts[1]:
                        keys.append(parts[1])
    print(f"DEBUG: Loaded {len(keys)} API keys", flush=True)
    return keys

CEREBRAS_KEYS = load_cerebras_keys()

# ============================================================
# éšæ®µ 1: é æƒæå»é‡ (å–®ç·šç¨‹ï¼Œç„¡ç«¶æ…‹æ¢ä»¶)
# ============================================================

def extract_transcript(content: str) -> str:
    """æå–é€å­—ç¨¿å…§å®¹ç”¨æ–¼ hash è¨ˆç®—"""
    # å˜—è©¦å¤šç¨®é€å­—ç¨¿æ¨™é¡Œæ ¼å¼
    patterns = [
        r'##\s*åŸå§‹é€å­—ç¨¿\s*\n(.+?)(?=\n##|\Z)',
        r'##\s*å®Œæ•´é€å­—ç¨¿\s*\n(.+?)(?=\n##|\Z)',
        r'##\s*Transcript\s*\n(.+?)(?=\n##|\Z)',
    ]
    for pattern in patterns:
        match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()
    # å˜—è©¦çŸ¥è­˜æå–å€å¡Š
    knowledge_match = re.search(r'## å•†æ¥­çŸ¥è­˜æå–\s*```markdown\s*(.+?)```', content, re.DOTALL)
    if knowledge_match:
        return knowledge_match.group(1).strip()
    return ""

def prescan_files(input_dir: Path) -> list:
    """
    é æƒææ‰€æœ‰æª”æ¡ˆï¼Œè¿”å›å”¯ä¸€å…§å®¹çš„æª”æ¡ˆåˆ—è¡¨
    åŒæ™‚æ¨™è¨˜è¢«è·³éçš„é‡è¤‡æª”æ¡ˆ
    """
    print("\nğŸ“Š éšæ®µ 1: é æƒæå»é‡...", flush=True)
    
    all_files = list(input_dir.rglob("*.md"))
    print(f"   ç™¼ç¾ {len(all_files)} å€‹ MD æª”æ¡ˆ", flush=True)
    
    hash_to_file = {}  # hash -> (file_path, transcript_length)
    unique_files = []
    duplicate_count = 0
    skipped_new_format = 0
    skipped_empty = 0
    
    for i, file_path in enumerate(all_files):
        if (i + 1) % 100 == 0:
            print(f"   æƒæé€²åº¦: {i+1}/{len(all_files)}", flush=True)
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # è·³éå·²æ˜¯æ–°æ ¼å¼çš„æª”æ¡ˆ
            if content.strip().startswith('---'):
                first_end = content.find('---', 3)
                if first_end > 0 and 'entities:' in content[:first_end]:
                    skipped_new_format += 1
                    continue
            
            # æå–é€å­—ç¨¿
            transcript = extract_transcript(content)
            if not transcript or len(transcript) < 50:
                skipped_empty += 1
                continue
            
            # è¨ˆç®— hash
            content_hash = hashlib.md5(transcript.encode('utf-8')).hexdigest()
            
            if content_hash in hash_to_file:
                # é‡è¤‡ï¼šé¸æ“‡é€å­—ç¨¿æ›´é•·çš„é‚£å€‹
                existing_file, existing_len = hash_to_file[content_hash]
                if len(transcript) > existing_len:
                    # æ›¿æ›
                    hash_to_file[content_hash] = (file_path, len(transcript))
                duplicate_count += 1
            else:
                hash_to_file[content_hash] = (file_path, len(transcript))
                
        except Exception as e:
            print(f"   âš ï¸ æƒæéŒ¯èª¤: {file_path.name} - {e}", flush=True)
    
    unique_files = [fp for fp, _ in hash_to_file.values()]
    
    print(f"\n   ğŸ“ˆ æƒæçµæœ:", flush=True)
    print(f"      å”¯ä¸€å…§å®¹: {len(unique_files)} å€‹", flush=True)
    print(f"      é‡è¤‡è·³é: {duplicate_count} å€‹", flush=True)
    print(f"      å·²è™•ç†æ ¼å¼: {skipped_new_format} å€‹", flush=True)
    print(f"      ç©º/ç„¡æ•ˆ: {skipped_empty} å€‹", flush=True)
    
    return unique_files

# ============================================================
# éšæ®µ 2: è™•ç†é‚è¼¯ (èˆ‡ä¹‹å‰é¡ä¼¼)
# ============================================================

def parse_old_format(content: str) -> dict:
    result = {
        'title': '', 'source': 'youtube', 'author': '', 
        'url': '', 'duration': '', 'process_date': '', 
        'knowledge_zh': '', 'transcript_en': ''
    }
    title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
    if title_match: result['title'] = title_match.group(1).strip()
    source_match = re.search(r'\*\*ä¾†æº\*\*:\s*(.+)', content)
    if source_match:
        parts = source_match.group(1).split('/')
        if len(parts) >= 2: result['author'] = parts[-1].strip()
    url_match = re.search(r'\*\*URL\*\*:\s*(https?://[^\s]+)', content)
    if url_match: result['url'] = url_match.group(1).strip()
    duration_match = re.search(r'\*\*æ™‚é•·\*\*:\s*(\d+:\d+)', content)
    if duration_match: result['duration'] = duration_match.group(1).strip()
    date_match = re.search(r'\*\*è™•ç†æ—¥æœŸ\*\*:\s*(\d{4}-\d{2}-\d{2})', content)
    if date_match: result['process_date'] = date_match.group(1).strip()
    
    transcript_match = re.search(r'##\s*(åŸå§‹é€å­—ç¨¿|å®Œæ•´é€å­—ç¨¿|Transcript)\s*\n(.+?)(?=\n##|\Z)', content, re.DOTALL | re.IGNORECASE)
    if transcript_match: result['transcript_en'] = transcript_match.group(2).strip()
    
    knowledge_match = re.search(r'## å•†æ¥­çŸ¥è­˜æå–\s*```markdown\s*(.+?)```', content, re.DOTALL)
    if knowledge_match: result['knowledge_zh'] = knowledge_match.group(1).strip()
    return result

def create_new_format(old_data: dict, knowledge_result: dict) -> str:
    yaml_lines = [
        "---",
        f"title: \"{old_data['title']}\"",
        "source: youtube",
        f"author: {old_data['author']}",
    ]
    if knowledge_result.get('guest'): yaml_lines.append(f"guest: {knowledge_result['guest']}")
    if old_data['url']: yaml_lines.append(f"url: {old_data['url']}")
    if old_data['duration']: yaml_lines.append(f"duration: \"{old_data['duration']}\"")
    if old_data['process_date']: 
        year = old_data['process_date'].split('-')[0]
        yaml_lines.append(f"content_year: {year}")
    
    yaml_lines.append(f"processed_at: {datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}")
    
    keywords = knowledge_result.get('keywords', [])
    if keywords: yaml_lines.append(f"keywords: [{', '.join(keywords[:10])}]")
    
    summary = knowledge_result.get('summary', '')
    if summary: yaml_lines.append(f'summary: "{summary.replace(chr(10), " ").replace(chr(34), chr(39))[:200]}"')
    
    entities = knowledge_result.get('entities', [])
    if entities: yaml_lines.append(f"entities: [{', '.join(entities[:8])}]")
    
    tags = knowledge_result.get('tags', [])
    if tags: yaml_lines.append(f"tags: [{', '.join(tags[:5])}]")
    
    yaml_lines.append("---")
    
    md_parts = [
        "\n".join(yaml_lines),
        "", "## é€å­—ç¨¿å…¨æ–‡", "",
        old_data['transcript_en'] or "_ï¼ˆç„¡è‹±æ–‡é€å­—ç¨¿ï¼‰_",
        "", "---", "", "## AI çŸ¥è­˜æå–", "",
        knowledge_result.get('knowledge', old_data['knowledge_zh']) or "_ï¼ˆç„¡çŸ¥è­˜æå–çµæœï¼‰_",
    ]
    return '\n'.join(md_parts)

def call_cerebras_api(text: str, video_info: dict, api_key: str) -> dict:
    from openai import OpenAI
    client = OpenAI(api_key=api_key, base_url="https://api.cerebras.ai/v1")
    
    ontology_path = Path.home() / "R2R/config/ontology/solo_entrepreneur_synonyms.json"
    tags_path = Path.home() / "R2R/config/ontology/solo_entrepreneur_tags.yaml"
    
    entities_hint, tags_hint = "", ""
    try:
        with open(ontology_path, 'r') as f:
            entities_hint = ", ".join(list(json.load(f).keys())[:80])
    except: pass
    try:
        with open(tags_path, 'r') as f:
            dims = yaml.safe_load(f).get('dimensions', {}).values()
            cats = [c for d in dims for c in d.get('categories', {}).values()]
            tags_hint = ", ".join([t for c in cats for t in c.get('tags', [])][:40])
    except: pass
    
    prompt = f"""åˆ†æä»¥ä¸‹å…§å®¹ï¼Œæå–ä¸€äººå…¬å¸å‰µæ¥­ç›¸é—œçš„çŸ¥è­˜ã€‚

æ¨™é¡Œï¼š{video_info.get('title', '')}
é »é“ï¼š{video_info.get('channel', '')}

å…§å®¹ï¼š
{text[:6000]}

è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰å›ç­”ï¼Œä¸¦åš´æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š

[KEYWORDS]
åˆ—å‡º 5-8 å€‹é—œéµå­—ï¼Œé€—è™Ÿåˆ†éš”

[SUMMARY]
ä¸€æ®µ 150 å­—ä»¥å…§çš„æ‘˜è¦

[ENTITIES]
å¾ä»¥ä¸‹é è¨­æ¸…å–®ä¸­é¸æ“‡ 5-8 å€‹æœ€ç›¸é—œçš„å¯¦é«”ï¼ˆåš´ç¦å‰µå»ºæ–°é …ç›®ï¼‰ï¼š
{entities_hint}

[TAGS]
å¾ä»¥ä¸‹é è¨­æ¸…å–®ä¸­é¸æ“‡ 3-5 å€‹æœ€ç›¸é—œçš„æ¨™ç±¤ï¼ˆåš´ç¦å‰µå»ºæ–°é …ç›®ï¼‰ï¼š
{tags_hint}

[GUEST]
è¨ªè«‡å˜‰è³“å§“åï¼ˆè‹¥ç„¡å‰‡ç•™ç©ºï¼‰

[KNOWLEDGE]
æå–çš„æ ¸å¿ƒçŸ¥è­˜å…§å®¹ï¼ˆmarkdownæ ¼å¼ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼‰"""

    response = client.chat.completions.create(
        model="qwen-3-235b-a22b-instruct-2507",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€äººå…¬å¸å‰µæ¥­çŸ¥è­˜æå–å°ˆå®¶ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨èªï¼‰å›ç­”ã€‚"},
            {"role": "user", "content": prompt}
        ],
        max_tokens=2500,
        temperature=0.3
    )
    result_text = response.choices[0].message.content
    result = {}
    
    for k in ['KEYWORDS', 'SUMMARY', 'ENTITIES', 'TAGS', 'GUEST']:
        m = re.search(fr'\[{k}\]\s*(.+?)(?=\[|\Z)', result_text, re.DOTALL)
        if m: result[k.lower()] = m.group(1).strip()
    
    m_kn = re.search(r'\[KNOWLEDGE\]\s*(.+?)(?=\Z)', result_text, re.DOTALL)
    if m_kn: result['knowledge'] = m_kn.group(1).strip()
    
    if 'keywords' in result: result['keywords'] = [x.strip() for x in result['keywords'].split(',')]
    if 'entities' in result: result['entities'] = [x.strip() for x in result['entities'].split(',')]
    if 'tags' in result: result['tags'] = [x.strip() for x in result['tags'].split(',')]
    
    return result

# ============================================================
# éšæ®µ 3: è‡ªé©æ‡‰æ§åˆ¶ & è™•ç†
# ============================================================

class AdaptiveController:
    def __init__(self):
        self.lock = threading.Lock()
        self.current_delay = INITIAL_DELAY
        self.api_key_index = 0
        self.exhausted_keys = set()
        self.success_count = 0
        self.error_count = 0
        
    def get_api_key(self):
        with self.lock:
            available = [k for k in CEREBRAS_KEYS if k not in self.exhausted_keys]
            if not available: return None
            key = available[self.api_key_index % len(available)]
            self.api_key_index += 1
            return key
            
    def report_success(self):
        with self.lock:
            self.success_count += 1
            self.error_count = 0
            if self.success_count > SUCCESS_THRESHOLD:
                self.current_delay = max(INITIAL_DELAY, self.current_delay / BACKOFF_FACTOR)
                self.success_count = 0
                
    def report_error(self, is_rate_limit: bool):
        with self.lock:
            self.error_count += 1
            self.current_delay = min(MAX_DELAY, self.current_delay * BACKOFF_FACTOR)
                
    def mark_key_exhausted(self, key):
        with self.lock:
            self.exhausted_keys.add(key)
            print(f"âŒ API Key è€—ç›¡: {key[:8]}...", flush=True)

    def wait(self):
        time.sleep(self.current_delay + random.uniform(0, 0.5))

class ProgressTracker:
    def __init__(self, progress_file: Path):
        self.progress_file = progress_file
        self.lock = threading.Lock()
        self.processed = set()
        self.load()
    
    def load(self):
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r') as f:
                    data = json.load(f)
                    self.processed = set(data.get('processed', []))
            except: pass
            
    def save(self):
        with self.lock:
            with open(self.progress_file, 'w') as f:
                json.dump({
                    'processed': list(self.processed),
                    'last_update': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
                
    def mark_done(self, file_path: str):
        with self.lock:
            self.processed.add(file_path)
            if len(self.processed) % 10 == 0:
                self.save()
                
    def is_done(self, file_path: str) -> bool:
        return file_path in self.processed

def process_file(file_path: Path, controller: AdaptiveController, progress: ProgressTracker) -> bool:
    file_key = str(file_path)
    if progress.is_done(file_key): 
        return None
    
    controller.wait()
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
                
        old_data = parse_old_format(content)
        text = old_data['transcript_en'] or old_data['knowledge_zh']
        
        if not text:
            progress.mark_done(file_key)
            return None
            
        video_info = {'title': old_data['title'], 'channel': old_data['author'], 'url': old_data['url']}
        
        max_retries = 5
        for attempt in range(max_retries):
            api_key = controller.get_api_key()
            if not api_key: return False
            
            try:
                result = call_cerebras_api(text, video_info, api_key)
                if old_data['knowledge_zh'] and not result.get('knowledge'):
                    result['knowledge'] = old_data['knowledge_zh']
                    
                new_content = create_new_format(old_data, result)
                
                rel_path = file_path.relative_to(INPUT_DIR)
                out_path = OUTPUT_DIR / rel_path
                out_path.parent.mkdir(parents=True, exist_ok=True)
                with open(out_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                    
                progress.mark_done(file_key)
                controller.report_success()
                print(f"  âœ… {file_path.name[:40]}...", flush=True)
                return True
                
            except Exception as e:
                err_msg = str(e).lower()
                is_rate_limit = '429' in err_msg or 'rate limit' in err_msg or 'quota' in err_msg
                
                if is_rate_limit:
                    controller.report_error(True)
                    controller.mark_key_exhausted(api_key)
                    if not controller.get_api_key():
                        print("âŒ æ‰€æœ‰ API Keys è€—ç›¡", flush=True)
                        return False
                else:
                    controller.report_error(False)
                    print(f"  âš ï¸ éŒ¯èª¤ ({attempt+1}/{max_retries}): {e}", flush=True)
                    
                time.sleep(controller.current_delay * (attempt + 1))
                
        return False
    except Exception as e:
        print(f"  âŒ åš´é‡éŒ¯èª¤: {file_path.name} - {e}", flush=True)
        return False

def main():
    if not CEREBRAS_KEYS:
        print("âŒ ç„¡å¯ç”¨ API Keys", flush=True)
        return
    
    # æ¸…é™¤èˆŠçš„é€²åº¦æª”æ¡ˆä»¥ç¢ºä¿å…¨æ–°é–‹å§‹
    if PROGRESS_FILE.exists():
        PROGRESS_FILE.unlink()
        print("ğŸ—‘ï¸ å·²æ¸…é™¤èˆŠé€²åº¦æª”æ¡ˆ", flush=True)
    
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
    # éšæ®µ 1: é æƒæå»é‡
    unique_files = prescan_files(INPUT_DIR)
    
    if not unique_files:
        print("âŒ ç„¡å¯è™•ç†çš„æª”æ¡ˆ", flush=True)
        return
    
    # éšæ®µ 2: å¤šç·šç¨‹è™•ç†
    print(f"\nğŸ“Š éšæ®µ 2: å¤šç·šç¨‹è™•ç† ({MAX_THREADS} ç·šç¨‹)...", flush=True)
    
    progress = ProgressTracker(PROGRESS_FILE)
    controller = AdaptiveController()
    
    pending_files = [f for f in unique_files if not progress.is_done(str(f))]
    print(f"   å¾…è™•ç†: {len(pending_files)} å€‹å”¯ä¸€å…§å®¹æª”æ¡ˆ", flush=True)
    
    success_count = 0
    error_count = 0
    
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        futures = {executor.submit(process_file, f, controller, progress): f for f in pending_files}
        
        for future in as_completed(futures):
            result = future.result()
            if result is True:
                success_count += 1
            elif result is False:
                error_count += 1
            
            if not controller.get_api_key():
                print("ğŸ åœæ­¢ï¼šç„¡å¯ç”¨ API Keys", flush=True)
                break

    progress.save()
    
    print("\n" + "=" * 70, flush=True)
    print("âœ… è™•ç†å®Œæˆ", flush=True)
    print(f"   æˆåŠŸ: {success_count}", flush=True)
    print(f"   éŒ¯èª¤: {error_count}", flush=True)
    print(f"   è¼¸å‡ºç›®éŒ„: {OUTPUT_DIR}", flush=True)
    print("=" * 70, flush=True)

if __name__ == "__main__":
    main()
