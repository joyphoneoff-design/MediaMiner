#!/usr/bin/env python3
"""
å°ç´…æ›¸çˆ¬èŸ²
æ“·å–å°ç´…æ›¸ç”¨æˆ¶ç­†è¨˜å’Œå½±ç‰‡
"""

import os
import re
import json
import subprocess
from pathlib import Path
from typing import List, Dict, Optional
import requests
from urllib.parse import urlparse, parse_qs

class XiaohongshuScraper:
    """å°ç´…æ›¸çˆ¬èŸ²é¡"""
    
    def __init__(self, output_dir: str = "~/Documents/MediaMiner_Data/raw"):
        self.output_dir = Path(output_dir).expanduser()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def resolve_short_url(self, short_url: str) -> Optional[str]:
        """
        è§£æçŸ­ç¶²å€åˆ°å®Œæ•´ URL
        
        Args:
            short_url: xhslink.com çŸ­ç¶²å€
            
        Returns:
            å®Œæ•´çš„å°ç´…æ›¸ URL
        """
        try:
            # è·Ÿéš¨é‡å®šå‘
            response = requests.head(short_url, allow_redirects=True, timeout=10)
            return response.url
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è§£æçŸ­ç¶²å€: {e}")
            return None
    
    def extract_note_id(self, url: str) -> Optional[str]:
        """
        å¾ URL æå–ç­†è¨˜ ID
        
        Args:
            url: å°ç´…æ›¸ URL
            
        Returns:
            ç­†è¨˜ ID
        """
        # ç­†è¨˜ URL æ ¼å¼: xiaohongshu.com/explore/xxx æˆ– discovery/item/xxx
        patterns = [
            r'/explore/([a-zA-Z0-9]+)',
            r'/discovery/item/([a-zA-Z0-9]+)',
            r'/note/([a-zA-Z0-9]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        return None
    
    def extract_user_id(self, url: str) -> Optional[str]:
        """
        å¾ URL æå–ç”¨æˆ¶ ID
        
        Args:
            url: å°ç´…æ›¸ç”¨æˆ¶é é¢ URL
            
        Returns:
            ç”¨æˆ¶ ID
        """
        # ç”¨æˆ¶é é¢æ ¼å¼: xiaohongshu.com/user/profile/xxx
        match = re.search(r'/user/profile/([a-zA-Z0-9]+)', url)
        if match:
            return match.group(1)
        return None
    
    def download_video_with_ytdlp(self, url: str) -> Dict:
        """
        ä½¿ç”¨ yt-dlp ä¸‹è¼‰å°ç´…æ›¸å½±ç‰‡
        
        Args:
            url: ç­†è¨˜æˆ–å½±ç‰‡ URL
            
        Returns:
            ä¸‹è¼‰çµæœ
        """
        try:
            # å˜—è©¦ä¸‹è¼‰å½±ç‰‡
            cmd = [
                "yt-dlp",
                "--write-info-json",
                "--write-subs",
                "--sub-langs", "all",
                "-o", str(self.output_dir / "%(title)s.%(ext)s"),
                url
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            if result.returncode == 0:
                return {
                    'success': True,
                    'message': result.stdout
                }
            else:
                return {
                    'success': False,
                    'error': result.stderr
                }
                
        except subprocess.TimeoutExpired:
            return {'success': False, 'error': 'Download timeout'}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_note_content_via_api(self, note_url: str) -> Optional[Dict]:
        """
        å˜—è©¦é€é API ç²å–ç­†è¨˜å…§å®¹
        (éœ€è¦é€²ä¸€æ­¥ç ”ç©¶å°ç´…æ›¸ API)
        
        Args:
            note_url: ç­†è¨˜ URL
            
        Returns:
            ç­†è¨˜å…§å®¹
        """
        # å°ç´…æ›¸æœ‰åçˆ¬æ©Ÿåˆ¶ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ Crawl4AI
        # æˆ–è€…éœ€è¦ç”¨æˆ¶æ‰‹å‹•æˆæ¬Š
        return None
    
    def scrape_with_crawl4ai(self, url: str) -> Optional[str]:
        """
        ä½¿ç”¨ Crawl4AI çˆ¬å–é é¢å…§å®¹
        
        Args:
            url: é é¢ URL
            
        Returns:
            é é¢å…§å®¹ (Markdown)
        """
        try:
            from crawl4ai import AsyncWebCrawler
            import asyncio
            
            async def crawl():
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=url)
                    return result.markdown
            
            return asyncio.run(crawl())
        except ImportError:
            print("âš ï¸ Crawl4AI æœªå®‰è£")
            return None
        except Exception as e:
            print(f"âš ï¸ Crawl4AI éŒ¯èª¤: {e}")
            return None
    
    def get_user_notes(self, url: str, max_notes: int = 0) -> List[Dict]:
        """
        ç²å–ç”¨æˆ¶æ‰€æœ‰ç­†è¨˜åˆ—è¡¨ (é¡ä¼¼ YouTube get_channel_videos)
        
        Args:
            url: å°ç´…æ›¸ URL (æ”¯æŒçŸ­ç¶²å€ xhslink.com)
            max_notes: æœ€å¤§ç­†è¨˜æ•¸ (0 = å…¨éƒ¨)
            
        Returns:
            [{'title': ..., 'url': ..., 'note_id': ..., 'type': 'video'|'image', ...}]
        """
        print(f"ğŸ“± ç²å–å°ç´…æ›¸ç”¨æˆ¶ç­†è¨˜åˆ—è¡¨...")
        
        # Step 1: è§£æ URL ç²å–ç”¨æˆ¶ ID
        full_url = self._resolve_to_profile_url(url)
        if not full_url:
            print("âŒ ç„¡æ³•è§£æ URL")
            return []
        
        user_id = self.extract_user_id(full_url)
        if not user_id:
            print(f"âŒ ç„¡æ³•å¾ URL æå–ç”¨æˆ¶ ID: {full_url}")
            return []
        
        print(f"   ç”¨æˆ¶ ID: {user_id}")
        
        # Step 2: å„ªå…ˆä½¿ç”¨ CDP (éœ€è¦ Chrome Debug æ¨¡å¼)
        notes = self._fetch_notes_via_cdp(full_url, max_notes)
        
        if not notes:
            # å‚™ç”¨æ–¹æ¡ˆ 1: ä½¿ç”¨ API
            print("   å˜—è©¦å‚™ç”¨æ–¹æ¡ˆ: API...")
            notes = self._fetch_notes_via_api(user_id, max_notes)
        
        if not notes:
            # å‚™ç”¨æ–¹æ¡ˆ 2: ä½¿ç”¨ç¶²é çˆ¬å–
            print("   å˜—è©¦å‚™ç”¨æ–¹æ¡ˆ 2: ç¶²é çˆ¬å–...")
            notes = self._fetch_notes_via_web(full_url, max_notes)
        
        if not notes:
            # å‚™ç”¨æ–¹æ¡ˆ 3: ä½¿ç”¨ Playwright ç€è¦½å™¨è‡ªå‹•åŒ–
            print("   å˜—è©¦å‚™ç”¨æ–¹æ¡ˆ 3: Playwright ç€è¦½å™¨...")
            notes = self._fetch_notes_via_playwright(full_url, max_notes)
        
        print(f"   âœ… æ‰¾åˆ° {len(notes)} å€‹ç­†è¨˜")
        return notes
    
    def _resolve_to_profile_url(self, url: str) -> Optional[str]:
        """è§£æçŸ­ç¶²å€åˆ°å®Œæ•´ç”¨æˆ¶é é¢ URL"""
        # å¦‚æœå·²ç¶“æ˜¯å®Œæ•´ URL
        if 'xiaohongshu.com/user/profile' in url:
            return url
        
        # ä½¿ç”¨ yt-dlp è§£æçŸ­ç¶²å€ (å®ƒæœƒè·Ÿéš¨é‡å®šå‘)
        try:
            import subprocess
            result = subprocess.run(
                ['yt-dlp', '--dump-json', url],
                capture_output=True, text=True, timeout=30
            )
            # yt-dlp æœƒè¼¸å‡ºéŒ¯èª¤ä¿¡æ¯ä¸­åŒ…å«å®Œæ•´ URL
            if 'xiaohongshu.com/user/profile' in result.stderr:
                import re
                match = re.search(r'(https://www\.xiaohongshu\.com/user/profile/[^\s\?]+)', result.stderr)
                if match:
                    return match.group(1)
        except Exception as e:
            print(f"   âš ï¸ yt-dlp è§£æå¤±æ•—: {e}")
        
        # å‚™ç”¨: ç›´æ¥ HEAD è«‹æ±‚
        try:
            response = requests.head(url, allow_redirects=True, timeout=10)
            if 'xiaohongshu.com' in response.url:
                return response.url
        except:
            pass
        
        return None
    
    def _fetch_notes_via_api(self, user_id: str, max_notes: int = 0) -> List[Dict]:
        """ä½¿ç”¨å°ç´…æ›¸ API ç²å–ç­†è¨˜åˆ—è¡¨"""
        notes = []
        cursor = ""
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'Origin': 'https://www.xiaohongshu.com',
            'Referer': f'https://www.xiaohongshu.com/user/profile/{user_id}',
        }
        
        # å°ç´…æ›¸ web API endpoint
        api_url = f"https://edith.xiaohongshu.com/api/sns/web/v1/user_posted"
        
        try:
            for page in range(20):  # æœ€å¤š 20 é 
                params = {
                    'num': 30,
                    'cursor': cursor,
                    'user_id': user_id,
                    'image_formats': 'jpg,webp,avif'
                }
                
                response = requests.get(api_url, headers=headers, params=params, timeout=15)
                
                if response.status_code != 200:
                    print(f"   âš ï¸ API è¿”å› {response.status_code}")
                    break
                
                data = response.json()
                
                if not data.get('success'):
                    break
                
                items = data.get('data', {}).get('notes', [])
                if not items:
                    break
                
                for item in items:
                    note = {
                        'title': item.get('display_title', 'ç„¡æ¨™é¡Œ'),
                        'note_id': item.get('note_id'),
                        'url': f"https://www.xiaohongshu.com/explore/{item.get('note_id')}",
                        'type': item.get('type', 'normal'),  # normal=åœ–ç‰‡, video=å½±ç‰‡
                        'cover': item.get('cover', {}).get('url', ''),
                        'likes': item.get('liked_count', 0),
                        'user': item.get('user', {}).get('nickname', ''),
                    }
                    notes.append(note)
                    
                    if max_notes > 0 and len(notes) >= max_notes:
                        return notes
                
                cursor = data.get('data', {}).get('cursor', '')
                if not cursor or not data.get('data', {}).get('has_more'):
                    break
                    
        except Exception as e:
            print(f"   âš ï¸ API è«‹æ±‚å¤±æ•—: {e}")
        
        return notes
    
    def _fetch_notes_via_web(self, profile_url: str, max_notes: int = 0) -> List[Dict]:
        """å‚™ç”¨: ä½¿ç”¨ç¶²é çˆ¬å–ç²å–ç­†è¨˜åˆ—è¡¨"""
        notes = []
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        }
        
        try:
            response = requests.get(profile_url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                # å¾ HTML ä¸­æå–ç­†è¨˜è³‡è¨Š
                import re
                
                # æŸ¥æ‰¾ç­†è¨˜é€£çµ
                note_pattern = r'/explore/([a-zA-Z0-9]+)'
                note_ids = list(set(re.findall(note_pattern, response.text)))
                
                for note_id in note_ids[:max_notes if max_notes > 0 else len(note_ids)]:
                    notes.append({
                        'title': f'ç­†è¨˜ {note_id[:8]}...',
                        'note_id': note_id,
                        'url': f'https://www.xiaohongshu.com/explore/{note_id}',
                        'type': 'unknown',
                        'cover': '',
                        'likes': 0,
                        'user': '',
                    })
                    
        except Exception as e:
            print(f"   âš ï¸ ç¶²é çˆ¬å–å¤±æ•—: {e}")
        
        return notes
    
    def _fetch_notes_via_playwright(self, profile_url: str, max_notes: int = 0) -> List[Dict]:
        """ä½¿ç”¨ Playwright ç€è¦½å™¨è‡ªå‹•åŒ–ç²å–ç­†è¨˜åˆ—è¡¨"""
        notes = []
        
        try:
            from playwright.sync_api import sync_playwright
            import time
            
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                context = browser.new_context(
                    user_agent='Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1',
                    viewport={'width': 390, 'height': 844}
                )
                page = context.new_page()
                
                # è¨ªå•ç”¨æˆ¶é é¢
                page.goto(profile_url, wait_until='networkidle', timeout=30000)
                time.sleep(2)  # ç­‰å¾…å‹•æ…‹å…§å®¹è¼‰å…¥
                
                # æ»¾å‹•è¼‰å…¥æ›´å¤šç­†è¨˜
                for _ in range(3):
                    page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                    time.sleep(1)
                
                # æå–ç­†è¨˜è³‡è¨Š
                content = page.content()
                
                # å¾ HTML ä¸­æå–ç­†è¨˜é€£çµå’Œæ¨™é¡Œ
                import re
                
                # æŸ¥æ‰¾ç­†è¨˜é€£çµ
                note_pattern = r'/explore/([a-zA-Z0-9]+)'
                note_ids = list(set(re.findall(note_pattern, content)))
                
                # å˜—è©¦æå–æ¨™é¡Œ
                title_pattern = r'<span[^>]*class="[^"]*title[^"]*"[^>]*>([^<]+)</span>'
                titles = re.findall(title_pattern, content)
                
                for i, note_id in enumerate(note_ids[:max_notes if max_notes > 0 else len(note_ids)]):
                    title = titles[i] if i < len(titles) else f'ç­†è¨˜ {note_id[:8]}...'
                    notes.append({
                        'title': title,
                        'note_id': note_id,
                        'url': f'https://www.xiaohongshu.com/explore/{note_id}',
                        'type': 'unknown',
                        'cover': '',
                        'likes': 0,
                        'user': '',
                    })
                
                browser.close()
                
        except ImportError:
            print("   âš ï¸ Playwright æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ: pip install playwright && playwright install chromium")
        except Exception as e:
            print(f"   âš ï¸ Playwright çˆ¬å–å¤±æ•—: {e}")
        
        return notes
    
    def _fetch_notes_via_cdp(self, profile_url: str, max_notes: int = 0) -> List[Dict]:
        """ä½¿ç”¨ Chrome Debug Protocol é€£æ¥å·²ç™»å…¥çš„ç€è¦½å™¨ç²å–ç­†è¨˜åˆ—è¡¨"""
        notes = []
        
        try:
            import socket
            # æª¢æŸ¥ Chrome Debug ç«¯å£æ˜¯å¦å¯ç”¨
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            result = sock.connect_ex(('localhost', 9222))
            sock.close()
            
            if result != 0:
                print("   âš ï¸ Chrome æœªåœ¨ Debug æ¨¡å¼é‹è¡Œ (ç«¯å£ 9222)")
                return notes
            
            from playwright.sync_api import sync_playwright
            import time
            
            print("   ğŸ”— é€£æ¥åˆ° Chrome Debug Protocol...")
            
            with sync_playwright() as p:
                browser = p.chromium.connect_over_cdp('http://localhost:9222')
                context = browser.contexts[0] if browser.contexts else None
                
                if not context:
                    print("   âš ï¸ ç„¡æ³•ç²å–ç€è¦½å™¨ä¸Šä¸‹æ–‡")
                    return notes
                
                page = context.new_page()
                
                print(f"   ğŸ“± è¨ªå•ç”¨æˆ¶é é¢...")
                page.goto(profile_url, wait_until='load', timeout=30000)
                time.sleep(3)
                
                # æ»¾å‹•è¼‰å…¥æ›´å¤šå…§å®¹
                scroll_count = 10 if max_notes == 0 else max(3, max_notes // 10)
                for i in range(scroll_count):
                    page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
                    time.sleep(1.5)
                    print(f"   ğŸ“œ æ»¾å‹• {i+1}/{scroll_count}...")
                
                content = page.content()
                page.close()
                
                # æå–ç­†è¨˜é€£çµ
                note_pattern = r'/explore/([a-zA-Z0-9]+)'
                note_ids = list(dict.fromkeys(re.findall(note_pattern, content)))  # ä¿æŒé †åºå»é‡
                
                # å˜—è©¦æå–æ¨™é¡Œ (å¾ DOM ä¸­)
                title_pattern = r'class="[^"]*title[^"]*"[^>]*>([^<]+)<'
                titles = re.findall(title_pattern, content)
                
                for i, note_id in enumerate(note_ids):
                    if max_notes > 0 and len(notes) >= max_notes:
                        break
                    
                    title = titles[i] if i < len(titles) else f'ç­†è¨˜ {note_id[:8]}...'
                    notes.append({
                        'title': title,
                        'note_id': note_id,
                        'url': f'https://www.xiaohongshu.com/explore/{note_id}',
                        'type': 'video',
                        'cover': '',
                        'likes': 0,
                        'user': '',
                    })
                
                print(f"   âœ… é€šé CDP æ‰¾åˆ° {len(notes)} å€‹ç­†è¨˜")
                
        except ImportError:
            print("   âš ï¸ Playwright æœªå®‰è£")
        except Exception as e:
            print(f"   âš ï¸ CDP é€£æ¥å¤±æ•—: {e}")
        
        return notes
    
    def process_user_profile(self, profile_url: str, max_notes: int = 10) -> List[Dict]:
        """è™•ç†ç”¨æˆ¶å€‹äººé é¢ (ä¿ç•™å‘å¾Œå…¼å®¹)"""
        return self.get_user_notes(profile_url, max_notes)


def test_xiaohongshu():
    """æ¸¬è©¦å°ç´…æ›¸çˆ¬èŸ²"""
    scraper = XiaohongshuScraper()
    
    print("ğŸ”´ å°ç´…æ›¸çˆ¬èŸ²æ¸¬è©¦")
    print("=" * 50)
    
    # æ¸¬è©¦çŸ­ç¶²å€è§£æ
    short_url = "https://xhslink.com/m/Arc4LKxLJBG"
    full_url = scraper.resolve_short_url(short_url)
    print(f"çŸ­ç¶²å€: {short_url}")
    print(f"å®Œæ•´URL: {full_url}")
    
    if full_url:
        user_id = scraper.extract_user_id(full_url)
        print(f"ç”¨æˆ¶ID: {user_id}")
    
    print("\nğŸ’¡ æç¤º:")
    print("1. å°ç´…æ›¸éœ€è¦ç›´æ¥çš„ç­†è¨˜é€£çµ (å« /explore/ æˆ– /note/)")
    print("2. å»ºè­°ä½¿ç”¨ç€è¦½å™¨ç²å–å…·é«”ç­†è¨˜ URL")
    print("3. ç¯„ä¾‹: https://www.xiaohongshu.com/explore/xxx")


if __name__ == "__main__":
    test_xiaohongshu()
