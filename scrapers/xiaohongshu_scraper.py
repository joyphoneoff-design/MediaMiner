#!/usr/bin/env python3
"""
å°ç´…æ›¸çˆ¬èŸ²
æ“·å–å°ç´…æ›¸ç”¨æˆ¶ç­†è¨˜å’Œå½±ç‰‡
"""

import os
import re
import json
import subprocess
from pathlib import Path
from typing import List, Dict, Optional
import requests
from urllib.parse import urlparse, parse_qs

class XiaohongshuScraper:
    """å°ç´…æ›¸çˆ¬èŸ²é¡"""
    
    def __init__(self, output_dir: str = "~/Documents/MediaMiner_Data/raw"):
        self.output_dir = Path(output_dir).expanduser()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def resolve_short_url(self, short_url: str) -> Optional[str]:
        """
        è§£æçŸ­ç¶²å€åˆ°å®Œæ•´ URL
        
        Args:
            short_url: xhslink.com çŸ­ç¶²å€
            
        Returns:
            å®Œæ•´çš„å°ç´…æ›¸ URL
        """
        try:
            # è·Ÿéš¨é‡å®šå‘
            response = requests.head(short_url, allow_redirects=True, timeout=10)
            return response.url
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è§£æçŸ­ç¶²å€: {e}")
            return None
    
    def extract_note_id(self, url: str) -> Optional[str]:
        """
        å¾ URL æå–ç­†è¨˜ ID
        
        Args:
            url: å°ç´…æ›¸ URL
            
        Returns:
            ç­†è¨˜ ID
        """
        # ç­†è¨˜ URL æ ¼å¼: xiaohongshu.com/explore/xxx æˆ– discovery/item/xxx
        patterns = [
            r'/explore/([a-zA-Z0-9]+)',
            r'/discovery/item/([a-zA-Z0-9]+)',
            r'/note/([a-zA-Z0-9]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        return None
    
    def extract_user_id(self, url: str) -> Optional[str]:
        """
        å¾ URL æå–ç”¨æˆ¶ ID
        
        Args:
            url: å°ç´…æ›¸ç”¨æˆ¶é é¢ URL
            
        Returns:
            ç”¨æˆ¶ ID
        """
        # ç”¨æˆ¶é é¢æ ¼å¼: xiaohongshu.com/user/profile/xxx
        match = re.search(r'/user/profile/([a-zA-Z0-9]+)', url)
        if match:
            return match.group(1)
        return None
    
    def download_video_with_ytdlp(self, url: str) -> Dict:
        """
        ä½¿ç”¨ yt-dlp ä¸‹è¼‰å°ç´…æ›¸å½±ç‰‡
        
        Args:
            url: ç­†è¨˜æˆ–å½±ç‰‡ URL
            
        Returns:
            ä¸‹è¼‰çµæœ
        """
        try:
            # å˜—è©¦ä¸‹è¼‰å½±ç‰‡
            cmd = [
                "yt-dlp",
                "--write-info-json",
                "--write-subs",
                "--sub-langs", "all",
                "-o", str(self.output_dir / "%(title)s.%(ext)s"),
                url
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            if result.returncode == 0:
                return {
                    'success': True,
                    'message': result.stdout
                }
            else:
                return {
                    'success': False,
                    'error': result.stderr
                }
                
        except subprocess.TimeoutExpired:
            return {'success': False, 'error': 'Download timeout'}
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def get_note_content_via_api(self, note_url: str) -> Optional[Dict]:
        """
        å˜—è©¦é€é API ç²å–ç­†è¨˜å…§å®¹
        (éœ€è¦é€²ä¸€æ­¥ç ”ç©¶å°ç´…æ›¸ API)
        
        Args:
            note_url: ç­†è¨˜ URL
            
        Returns:
            ç­†è¨˜å…§å®¹
        """
        # å°ç´…æ›¸æœ‰åçˆ¬æ©Ÿåˆ¶ï¼Œå¯èƒ½éœ€è¦ä½¿ç”¨ Crawl4AI
        # æˆ–è€…éœ€è¦ç”¨æˆ¶æ‰‹å‹•æˆæ¬Š
        return None
    
    def scrape_with_crawl4ai(self, url: str) -> Optional[str]:
        """
        ä½¿ç”¨ Crawl4AI çˆ¬å–é é¢å…§å®¹
        
        Args:
            url: é é¢ URL
            
        Returns:
            é é¢å…§å®¹ (Markdown)
        """
        try:
            from crawl4ai import AsyncWebCrawler
            import asyncio
            
            async def crawl():
                async with AsyncWebCrawler() as crawler:
                    result = await crawler.arun(url=url)
                    return result.markdown
            
            return asyncio.run(crawl())
        except ImportError:
            print("âš ï¸ Crawl4AI æœªå®‰è£")
            return None
        except Exception as e:
            print(f"âš ï¸ Crawl4AI éŒ¯èª¤: {e}")
            return None
    
    def process_user_profile(self, profile_url: str, max_notes: int = 10) -> List[Dict]:
        """
        è™•ç†ç”¨æˆ¶å€‹äººé é¢ï¼Œæå–ç­†è¨˜åˆ—è¡¨
        
        Args:
            profile_url: ç”¨æˆ¶é é¢ URL
            max_notes: æœ€å¤§ç­†è¨˜æ•¸
            
        Returns:
            è™•ç†çµæœåˆ—è¡¨
        """
        print(f"ğŸ“± è™•ç†å°ç´…æ›¸ç”¨æˆ¶é é¢: {profile_url}")
        
        # å˜—è©¦ä½¿ç”¨ Crawl4AI ç²å–é é¢
        content = self.scrape_with_crawl4ai(profile_url)
        
        if content:
            # å¾å…§å®¹ä¸­æå–ç­†è¨˜é€£çµ
            note_links = re.findall(r'https?://[^\s]+/explore/[a-zA-Z0-9]+', content)
            note_links = list(set(note_links))[:max_notes]
            
            print(f"   æ‰¾åˆ° {len(note_links)} å€‹ç­†è¨˜é€£çµ")
            
            results = []
            for link in note_links:
                result = self.download_video_with_ytdlp(link)
                results.append({
                    'url': link,
                    **result
                })
            
            return results
        else:
            print("   âš ï¸ ç„¡æ³•ç²å–ç”¨æˆ¶é é¢å…§å®¹")
            print("   ğŸ’¡ å»ºè­°ï¼šæ‰‹å‹•è¤‡è£½ç­†è¨˜é€£çµé€²è¡Œè™•ç†")
            return []


def test_xiaohongshu():
    """æ¸¬è©¦å°ç´…æ›¸çˆ¬èŸ²"""
    scraper = XiaohongshuScraper()
    
    print("ğŸ”´ å°ç´…æ›¸çˆ¬èŸ²æ¸¬è©¦")
    print("=" * 50)
    
    # æ¸¬è©¦çŸ­ç¶²å€è§£æ
    short_url = "https://xhslink.com/m/Arc4LKxLJBG"
    full_url = scraper.resolve_short_url(short_url)
    print(f"çŸ­ç¶²å€: {short_url}")
    print(f"å®Œæ•´URL: {full_url}")
    
    if full_url:
        user_id = scraper.extract_user_id(full_url)
        print(f"ç”¨æˆ¶ID: {user_id}")
    
    print("\nğŸ’¡ æç¤º:")
    print("1. å°ç´…æ›¸éœ€è¦ç›´æ¥çš„ç­†è¨˜é€£çµ (å« /explore/ æˆ– /note/)")
    print("2. å»ºè­°ä½¿ç”¨ç€è¦½å™¨ç²å–å…·é«”ç­†è¨˜ URL")
    print("3. ç¯„ä¾‹: https://www.xiaohongshu.com/explore/xxx")


if __name__ == "__main__":
    test_xiaohongshu()
